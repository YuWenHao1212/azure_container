name: Prompt A/B Testing

on:
  workflow_dispatch:
    inputs:
      task:
        description: 'Task to test'
        required: true
        type: choice
        options:
          - gap_analysis
          - keyword_extraction
          - index_calculation
          - resume_format
          - resume_tailor
      version_a:
        description: 'Version A (control)'
        required: true
        type: string
      version_b:
        description: 'Version B (treatment)'
        required: true
        type: string
      test_samples:
        description: 'Number of test samples'
        required: false
        type: number
        default: 10

env:
  PYTHONPATH: /home/runner/work/azure_container/azure_container

jobs:
  validate-versions:
    name: Validate Prompt Versions
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Check both versions exist
      run: |
        # Check Version A
        PROMPT_A="src/prompts/${{ github.event.inputs.task }}/v${{ github.event.inputs.version_a }}.yaml"
        if [ ! -f "$PROMPT_A" ]; then
          echo "❌ Version A not found: $PROMPT_A"
          exit 1
        fi
        echo "✅ Version A found: $PROMPT_A"
        
        # Check Version B
        PROMPT_B="src/prompts/${{ github.event.inputs.task }}/v${{ github.event.inputs.version_b }}.yaml"
        if [ ! -f "$PROMPT_B" ]; then
          echo "❌ Version B not found: $PROMPT_B"
          exit 1
        fi
        echo "✅ Version B found: $PROMPT_B"
        
  run-ab-test:
    name: Run A/B Test Comparison
    needs: validate-versions
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11.8'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-json-report matplotlib seaborn pandas
        
    - name: Create test script
      run: |
        cat > run_ab_test.py << 'EOF'
        import os
        import sys
        import json
        import time
        import asyncio
        from typing import Dict, List, Any
        
        # Add project root to path
        sys.path.insert(0, os.path.abspath('.'))
        
        async def test_prompt_version(task: str, version: str, test_data: Dict) -> Dict[str, Any]:
            """Test a specific prompt version."""
            # Set environment variable for the version
            env_var_name = f"{task.upper().replace('-', '_')}_PROMPT_VERSION"
            os.environ[env_var_name] = version
            
            results = {
                'version': version,
                'response_times': [],
                'token_counts': [],
                'quality_scores': [],
                'errors': []
            }
            
            try:
                if task == 'gap_analysis':
                    from src.services.gap_analysis_v2 import GapAnalysisServiceV2
                    from src.services.index_calculation_v2 import IndexCalculationServiceV2
                    
                    # Initialize services
                    gap_service = GapAnalysisServiceV2()
                    index_service = IndexCalculationServiceV2()
                    
                    # Run index calculation first
                    index_result = await index_service.calculate_with_analysis(
                        test_data['resume'],
                        test_data['job_description']
                    )
                    
                    # Run gap analysis
                    start_time = time.time()
                    result = await gap_service.analyze_with_context(
                        test_data['resume'],
                        test_data['job_description'],
                        index_result
                    )
                    response_time = time.time() - start_time
                    
                    results['response_times'].append(response_time)
                    
                    # Analyze response quality
                    overall_assessment = result.get('OverallAssessment', '')
                    word_count = len(overall_assessment.split())
                    results['quality_scores'].append({
                        'word_count': word_count,
                        'has_skill_gaps': '[Skill Gap]' in result.get('KeyGaps', ''),
                        'has_presentation_gaps': '[Presentation Gap]' in result.get('KeyGaps', ''),
                        'sections_complete': all([
                            result.get('CoreStrengths'),
                            result.get('KeyGaps'),
                            result.get('QuickImprovements'),
                            result.get('OverallAssessment')
                        ])
                    })
                    
                elif task == 'keyword_extraction':
                    from src.services.keyword_extraction import KeywordExtractionService
                    
                    service = KeywordExtractionService()
                    start_time = time.time()
                    result = await service.extract_keywords(test_data['job_description'])
                    response_time = time.time() - start_time
                    
                    results['response_times'].append(response_time)
                    results['quality_scores'].append({
                        'keyword_count': len(result.get('keywords', [])),
                        'has_categories': bool(result.get('categories', {}))
                    })
                    
            except Exception as e:
                results['errors'].append(str(e))
                
            return results
        
        async def main():
            task = "${{ github.event.inputs.task }}"
            version_a = "${{ github.event.inputs.version_a }}"
            version_b = "${{ github.event.inputs.version_b }}"
            num_samples = int("${{ github.event.inputs.test_samples }}")
            
            # Create test data based on task
            test_samples = []
            for i in range(num_samples):
                if task in ['gap_analysis', 'index_calculation']:
                    test_samples.append({
                        'resume': f"""
                        <h2>Professional Summary</h2>
                        <p>Experienced software engineer with {5+i} years of expertise in Python, 
                        JavaScript, and cloud technologies. Strong background in building 
                        scalable applications and leading technical teams.</p>
                        
                        <h2>Skills</h2>
                        <ul>
                        <li>Python, Django, FastAPI</li>
                        <li>JavaScript, React, Node.js</li>
                        <li>AWS, Docker, Kubernetes</li>
                        <li>PostgreSQL, MongoDB</li>
                        </ul>
                        
                        <h2>Experience</h2>
                        <p>Senior Software Engineer at Tech Corp (2020-Present)</p>
                        <ul>
                        <li>Led development of microservices architecture</li>
                        <li>Improved system performance by 40%</li>
                        <li>Mentored team of {3+i%3} junior developers</li>
                        </ul>
                        """,
                        'job_description': f"""
                        We are looking for a Senior Full Stack Developer with {6+i} years of experience.
                        
                        Required Skills:
                        - Expert in Python and JavaScript
                        - Experience with React and Node.js
                        - Strong knowledge of AWS services
                        - Proficient in Docker and Kubernetes
                        - Database design with PostgreSQL
                        - Experience with microservices architecture
                        - Team leadership experience
                        
                        Nice to have:
                        - Machine learning experience
                        - DevOps expertise
                        - Agile methodology
                        
                        Responsibilities:
                        - Design and implement scalable solutions
                        - Lead technical decisions
                        - Mentor junior developers
                        - Code review and quality assurance
                        """
                    })
                else:
                    test_samples.append({
                        'job_description': f"""
                        Senior Software Engineer Position - Sample {i+1}
                        
                        Required Skills:
                        - {5+i} years of Python experience
                        - Expertise in FastAPI and Django
                        - Cloud platforms (AWS, Azure)
                        - Docker and Kubernetes
                        - SQL and NoSQL databases
                        - REST API design
                        - Microservices architecture
                        
                        Responsibilities:
                        - Build scalable backend services
                        - Design system architecture
                        - Mentor team members
                        - Drive technical excellence
                        """
                    })
            
            # Run tests for both versions
            print(f"🧪 Running A/B test for {task}")
            print(f"Version A: {version_a}")
            print(f"Version B: {version_b}")
            print(f"Samples: {num_samples}")
            print("-" * 50)
            
            results_a = []
            results_b = []
            
            for i, sample in enumerate(test_samples):
                print(f"Testing sample {i+1}/{num_samples}...")
                
                # Test Version A
                result_a = await test_prompt_version(task, version_a, sample)
                results_a.append(result_a)
                
                # Test Version B
                result_b = await test_prompt_version(task, version_b, sample)
                results_b.append(result_b)
                
                # Add delay to avoid rate limiting
                await asyncio.sleep(2)
            
            # Analyze results
            print("\n" + "=" * 50)
            print("📊 A/B Test Results")
            print("=" * 50)
            
            # Calculate metrics for Version A
            avg_time_a = sum([r['response_times'][0] for r in results_a if r['response_times']]) / len(results_a)
            errors_a = sum([len(r['errors']) for r in results_a])
            
            # Calculate metrics for Version B
            avg_time_b = sum([r['response_times'][0] for r in results_b if r['response_times']]) / len(results_b)
            errors_b = sum([len(r['errors']) for r in results_b])
            
            print(f"\n📈 Version A ({version_a}):")
            print(f"  - Avg Response Time: {avg_time_a:.2f}s")
            print(f"  - Errors: {errors_a}")
            
            print(f"\n📈 Version B ({version_b}):")
            print(f"  - Avg Response Time: {avg_time_b:.2f}s")
            print(f"  - Errors: {errors_b}")
            
            # Quality comparison for gap analysis
            if task == 'gap_analysis' and results_a[0]['quality_scores']:
                avg_words_a = sum([s['quality_scores'][0]['word_count'] for s in results_a if s['quality_scores']]) / len(results_a)
                avg_words_b = sum([s['quality_scores'][0]['word_count'] for s in results_b if s['quality_scores']]) / len(results_b)
                
                print(f"\n📝 Content Quality:")
                print(f"  Version A - Avg Word Count: {avg_words_a:.0f}")
                print(f"  Version B - Avg Word Count: {avg_words_b:.0f}")
            
            # Determine winner
            print("\n" + "=" * 50)
            print("🏆 Recommendation:")
            
            if errors_b < errors_a:
                print(f"✅ Version B ({version_b}) - Fewer errors")
            elif errors_a < errors_b:
                print(f"✅ Version A ({version_a}) - Fewer errors")
            elif avg_time_b < avg_time_a * 0.9:  # 10% improvement threshold
                print(f"✅ Version B ({version_b}) - Faster by {((avg_time_a - avg_time_b) / avg_time_a * 100):.1f}%")
            elif avg_time_a < avg_time_b * 0.9:
                print(f"✅ Version A ({version_a}) - Faster by {((avg_time_b - avg_time_a) / avg_time_b * 100):.1f}%")
            else:
                print("🤝 No significant difference - Consider other factors")
            
            # Save detailed results
            with open('ab_test_results.json', 'w') as f:
                json.dump({
                    'task': task,
                    'version_a': version_a,
                    'version_b': version_b,
                    'results_a': results_a,
                    'results_b': results_b,
                    'summary': {
                        'avg_time_a': avg_time_a,
                        'avg_time_b': avg_time_b,
                        'errors_a': errors_a,
                        'errors_b': errors_b
                    }
                }, f, indent=2)
        
        if __name__ == "__main__":
            asyncio.run(main())
        EOF
        
    - name: Create .env file
      run: |
        cat > .env << EOF
        ENVIRONMENT=test
        LOG_LEVEL=INFO
        
        # Azure OpenAI
        AZURE_OPENAI_ENDPOINT=${{ secrets.AZURE_OPENAI_ENDPOINT }}
        AZURE_OPENAI_API_KEY=${{ secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_API_VERSION=2025-01-01-preview
        AZURE_OPENAI_GPT4_DEPLOYMENT=gpt-4.1-japan
        GPT41_MINI_JAPANEAST_DEPLOYMENT=gpt-4-1-mini-japaneast
        GPT41_MINI_JAPANEAST_ENDPOINT=${{ secrets.AZURE_OPENAI_ENDPOINT }}
        GPT41_MINI_JAPANEAST_API_KEY=${{ secrets.AZURE_OPENAI_API_KEY }}
        GPT41_MINI_JAPANEAST_API_VERSION=2025-01-01-preview
        
        # Embedding
        EMBEDDING_ENDPOINT=${{ secrets.EMBEDDING_ENDPOINT }}
        EMBEDDING_API_KEY=${{ secrets.AZURE_OPENAI_API_KEY }}
        COURSE_EMBEDDING_ENDPOINT=${{ secrets.COURSE_EMBEDDING_ENDPOINT }}
        COURSE_EMBEDDING_API_KEY=${{ secrets.AZURE_OPENAI_API_KEY }}
        
        # Model Selection
        LLM_MODEL_KEYWORDS=gpt-4.1-mini
        LLM_MODEL_GAP_ANALYSIS=gpt-4.1
        
        # Security
        JWT_SECRET_KEY=test-key
        CONTAINER_APP_API_KEY=test-key
        EOF
        
    - name: Run A/B test
      run: |
        python run_ab_test.py
        
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: ab-test-results
        path: ab_test_results.json
        
    - name: Create summary
      if: always()
      run: |
        echo "## 🧪 A/B Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Task**: ${{ github.event.inputs.task }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Version A**: ${{ github.event.inputs.version_a }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Version B**: ${{ github.event.inputs.version_b }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Samples**: ${{ github.event.inputs.test_samples }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f ab_test_results.json ]; then
          echo "### Results Summary" >> $GITHUB_STEP_SUMMARY
          
          # Extract key metrics using Python
          python -c "
import json
with open('ab_test_results.json') as f:
    data = json.load(f)
    s = data['summary']
    print(f'| Metric | Version A | Version B | Difference |')
    print(f'|--------|-----------|-----------|------------|')
    print(f'| Avg Response Time | {s[\"avg_time_a\"]:.2f}s | {s[\"avg_time_b\"]:.2f}s | {abs(s[\"avg_time_a\"] - s[\"avg_time_b\"]):.2f}s |')
    print(f'| Errors | {s[\"errors_a\"]} | {s[\"errors_b\"]} | {abs(s[\"errors_a\"] - s[\"errors_b\"])} |')
          " >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Triggered by**: ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
        echo "**Time**: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_STEP_SUMMARY
# Index Calculation and Gap Analysis V2 å¯¦ä½œæŒ‡å—

**æ–‡æª”ç‰ˆæœ¬**: 1.1.0  
**å»ºç«‹æ—¥æœŸ**: 2025-08-03  
**æ›´æ–°æ—¥æœŸ**: 2025-08-05  
**ç‹€æ…‹**: âœ… å¯¦ä½œå®Œæˆ

## ğŸ“‹ å¯¦ä½œæ¦‚è¿°

æœ¬æŒ‡å—æä¾› Index Calculation and Gap Analysis V2 çš„è©³ç´°å¯¦ä½œæ­¥é©Ÿï¼ŒåŒ…æ‹¬ç¨‹å¼ç¢¼ç¯„ä¾‹ã€é…ç½®èªªæ˜å’Œéƒ¨ç½²æµç¨‹ã€‚

## ğŸ¯ å¯¦ä½œç›®æ¨™èˆ‡é”æˆçµæœ

### æ•ˆèƒ½ç›®æ¨™èˆ‡å¯¦æ¸¬
| ç›®æ¨™ | åŸå§‹ç›®æ¨™ | å¯¦æ¸¬çµæœ | é”æˆç‹€æ³ |
|------|----------|----------|----------|
| P50 éŸ¿æ‡‰æ™‚é–“ | < 2 ç§’ | 19.009 ç§’ | âŒ éœ€å„ªåŒ– |
| P95 éŸ¿æ‡‰æ™‚é–“ | < 4 ç§’ | 24.892 ç§’ | âŒ éœ€å„ªåŒ– |
| è³‡æºæ± é‡ç”¨ç‡ | > 80% | 100% | âœ… è¶…è¶Šç›®æ¨™ |
| API å‘¼å«å„ªåŒ– | æ¸›å°‘ 50% | ä½¿ç”¨ LLM Factory çµ±ä¸€ç®¡ç† | âœ… é”æˆ |

### åŠŸèƒ½ç›®æ¨™é”æˆ
- âœ… å®Œå…¨å‘å¾Œç›¸å®¹ç¾æœ‰ API
- âœ… æ”¯æ´éƒ¨åˆ†çµæœè¿”å›ï¼ˆé€ééŒ¯èª¤è™•ç†ï¼‰
- âœ… å®Œæ•´çš„è¼¸å…¥é©—è­‰ï¼ˆ200å­—å…ƒæœ€å°é•·åº¦ã€èªè¨€ç™½åå–®ï¼‰
- âœ… çµ±ä¸€çš„ LLM ç®¡ç†ï¼ˆLLM Factoryï¼‰
- âœ… 100% æ¸¬è©¦è¦†è“‹ï¼ˆ42å€‹æ¸¬è©¦å…¨éƒ¨é€šéï¼‰

## ğŸ“ å°ˆæ¡ˆçµæ§‹ï¼ˆå¯¦éš›å¯¦ä½œï¼‰

```
src/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ v1/
â”‚       â””â”€â”€ endpoints/
â”‚           â””â”€â”€ index_cal_and_gap_analysis.py  # V2 API ç«¯é»
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ combined_analysis_v2.py               # çµ±ä¸€æœå‹™é¡åˆ¥
â”‚   â”œâ”€â”€ gap_analysis_v2.py                    # å‡ç´šç‰ˆ gap æœå‹™
â”‚   â”œâ”€â”€ llm_factory.py                        # ğŸš¨ çµ±ä¸€ LLM ç®¡ç†ï¼ˆæœ€é‡è¦ï¼‰
â”‚   â”œâ”€â”€ embedding_factory.py                  # çµ±ä¸€ Embedding ç®¡ç†
â”‚   â””â”€â”€ resource_pools/                       # è³‡æºæ± å¯¦ä½œ
â”‚       â”œâ”€â”€ llm_resource_pool.py              # LLM å®¢æˆ¶ç«¯æ± 
â”‚       â””â”€â”€ embedding_resource_pool.py        # Embedding å®¢æˆ¶ç«¯æ± 
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ validation.py                         # è¼¸å…¥é©—è­‰å·¥å…·
â””â”€â”€ tests/
    â”œâ”€â”€ unit/                                 # 20 å€‹å–®å…ƒæ¸¬è©¦
    â”œâ”€â”€ integration/                          # 17 å€‹æ•´åˆæ¸¬è©¦
    â”œâ”€â”€ performance/                          # 2 å€‹æ•ˆèƒ½æ¸¬è©¦
    â””â”€â”€ e2e/                                  # 3 å€‹ç«¯åˆ°ç«¯æ¸¬è©¦
```

## ğŸ› ï¸ æ ¸å¿ƒå¯¦ä½œ

### ğŸš¨ æœ€é‡è¦ï¼šLLM Factory çµ±ä¸€ç®¡ç†

**æ‰€æœ‰ LLM å‘¼å«å¿…é ˆé€šé LLM Factoryï¼é€™æ˜¯æœ¬æ¬¡å¯¦ä½œæœ€é—œéµçš„æ”¹é€²ï¼**

```python
# âŒ çµ•å°ç¦æ­¢ - æœƒå°è‡´ 500 éŒ¯èª¤ "deployment does not exist"
from openai import AsyncAzureOpenAI
from src.services.openai_client import get_azure_openai_client
client = get_azure_openai_client()  # ä½¿ç”¨éŒ¯èª¤çš„ deployment

# âœ… å”¯ä¸€æ­£ç¢ºçš„æ–¹å¼
from src.services.llm_factory import get_llm_client
client = get_llm_client(api_name="gap_analysis")  # è‡ªå‹•æ˜ å°„åˆ°æ­£ç¢º deployment
```

**ç‚ºä»€éº¼é€™å¾ˆé‡è¦**ï¼š
1. LLM Factory åŒ…å« DEPLOYMENT_MAP è‡ªå‹•è™•ç†æ¨¡å‹æ˜ å°„
2. `gpt4o-2` â†’ `gpt-4.1-japan`ï¼ˆå¯¦éš›éƒ¨ç½²åç¨±ï¼‰
3. é¿å… "deployment does not exist" éŒ¯èª¤
4. Claude Code ç¿’æ…£ç›´æ¥ä½¿ç”¨ OpenAI SDKï¼Œå¿…é ˆç³¾æ­£

### æ¶æ§‹æ¦‚è¦½ï¼ˆé¡¯ç¤ºæœå‹™ä¾è³´ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API è«‹æ±‚                                 â”‚
â”‚                 /index-cal-and-gap-analysis                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CombinedAnalysisServiceV2                      â”‚
â”‚         ï¼ˆçµ±ä¸€å”èª¿æœå‹™ï¼Œä½¿ç”¨è³‡æºæ± ç®¡ç†ï¼‰                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                         â”‚
              â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IndexCalculationServiceV2â”‚  â”‚                              â”‚
â”‚  ï¼ˆç¾æœ‰å„ªåŒ–ç‰ˆæœ¬ï¼‰        â”‚  â”‚                              â”‚
â”‚  - è¨ˆç®—ç›¸ä¼¼åº¦            â”‚  â”‚       ç­‰å¾… Index çµæœ        â”‚
â”‚  - é—œéµå­—åŒ¹é…åˆ†æ        â”œâ”€â”€â”¤                              â”‚
â”‚  è¼¸å‡ºï¼š                  â”‚  â”‚                              â”‚
â”‚  - matched_keywords      â”‚  â”‚                              â”‚
â”‚  - missing_keywords      â”‚  â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   GapAnalysisServiceV2       â”‚
                              â”‚  ï¼ˆä½¿ç”¨ Index çµæœä½œç‚ºè¼¸å…¥ï¼‰â”‚
                              â”‚  - æ¥æ”¶ matched_keywords    â”‚
                              â”‚  - æ¥æ”¶ missing_keywords    â”‚
                              â”‚  - ç”Ÿæˆå„ªå‹¢/å·®è·åˆ†æ        â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. API ç«¯é»å¯¦ä½œï¼ˆå¯¦éš›å®Œæˆç‰ˆæœ¬ï¼‰

```python
# src/api/v1/endpoints/index_cal_and_gap_analysis.py

from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, Optional
import time

from src.models.requests import CombinedAnalysisRequest
from src.models.responses import CombinedAnalysisResponse
from src.services.combined_analysis_v2 import CombinedAnalysisServiceV2
from src.utils.feature_flags import FeatureFlags
from src.utils.auth import verify_api_key

router = APIRouter()

# æœå‹™å¯¦ä¾‹
v1_service = CombinedAnalysisService()  # ç¾æœ‰æœå‹™
v2_service = CombinedAnalysisServiceV2()  # å„ªåŒ–ç‰ˆæœå‹™

@router.post(
    "/api/v1/index-cal-and-gap-analysis",
    response_model=CombinedAnalysisResponse,
    summary="è¨ˆç®—å±¥æ­·åŒ¹é…æŒ‡æ•¸ä¸¦é€²è¡Œå·®è·åˆ†æ",
    description="ä½¿ç”¨ feature flag æ§åˆ¶ V1/V2 å¯¦ä½œ"
)
async def calculate_index_and_analyze_gap(
    request: CombinedAnalysisRequest,
    api_key: str = Depends(verify_api_key)
) -> CombinedAnalysisResponse:
    """
    çµ„åˆåˆ†æ API with V2 å„ªåŒ–ï¼ˆé€é feature flagï¼‰ã€‚
    
    V2 æ”¹é€²ï¼š
    1. è³‡æºæ± ç®¡ç†æ¸›å°‘åˆå§‹åŒ–é–‹éŠ·
    2. å®Œå…¨ä¸¦è¡Œè™•ç†
    3. æ™ºèƒ½é‡è©¦ç­–ç•¥
    4. æ”¯æ´éƒ¨åˆ†çµæœ
    """
    
    start_time = time.time()
    
    # æ ¹æ“š feature flag é¸æ“‡å¯¦ä½œ
    if FeatureFlags.USE_V2_IMPLEMENTATION:
        service = v2_service
        version = "v2"
    else:
        service = v1_service
        version = "v1"
    
    try:
        # åŸ·è¡Œåˆ†æ
        result = await service.analyze(
                resume=request.resume,
                job_description=request.job_description,
                keywords=request.keywords,
                language=request.language,
                analysis_options=request.analysis_options
            )
            
            # è¨ˆç®—è™•ç†æ™‚é–“
            processing_time_ms = (time.time() - start_time) * 1000
            
            # å»ºæ§‹å›æ‡‰
            return CombinedAnalysisResponse(
                success=True,
                data={
                    **result,
                    "processing_time_ms": processing_time_ms,
                    "request_id": request_id
                }
            )
            
    except PartialFailureError as e:
        # éƒ¨åˆ†å¤±æ•—ï¼Œè¿”å›å¯ç”¨çµæœ
        return CombinedAnalysisResponse(
            success=False,
            data=e.partial_data,
            error={
                "code": "PARTIAL_FAILURE",
                "message": str(e),
                "details": e.details
            }
        )
        
    except Exception as e:
        # å®Œå…¨å¤±æ•—
        logger.error(f"Analysis failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "code": "ANALYSIS_ERROR",
                "message": "åˆ†ææœå‹™ç™¼ç”ŸéŒ¯èª¤",
                "request_id": request_id
            }
        )
```

### 2. çµ±ä¸€æœå‹™å¯¦ä½œï¼ˆå¯¦éš›å®Œæˆç‰ˆæœ¬ï¼‰

```python
# src/services/combined_analysis_v2.py

from typing import Dict, Any, List, Optional
import logging
import os

from src.services.index_calculation_v2 import IndexCalculationServiceV2  
from src.services.gap_analysis_v2 import GapAnalysisServiceV2
from src.services.llm_factory import get_llm_client  # ğŸš¨ é‡è¦ï¼šä½¿ç”¨ LLM Factory
from src.services.embedding_factory import get_embedding_client
from src.services.resource_pools.llm_resource_pool import LLMResourcePool
from src.services.resource_pools.embedding_resource_pool import EmbeddingResourcePool

class CombinedAnalysisServiceV2(BaseService):
    """
    çµ±ä¸€çš„å±¥æ­·åˆ†ææœå‹™ V2ï¼ˆç²¾ç°¡ç‰ˆï¼‰ã€‚
    
    ç‰¹é»ï¼š
    1. è³‡æºæ± ç®¡ç†æ¸›å°‘é–‹éŠ·
    2. å®Œå…¨ä¸¦è¡ŒåŸ·è¡Œ
    3. éƒ¨åˆ†çµæœæ”¯æ´
    4. æ™ºèƒ½é‡è©¦
    """
    
    def __init__(
        self,
        index_service: Optional[IndexCalculationServiceV2] = None,
        gap_service: Optional[GapAnalysisServiceV2] = None,
        resource_pool: Optional[ResourcePoolManager] = None,
        enable_partial_results: bool = True
    ):
        super().__init__()
        
        # æœå‹™ä¾è³´
        self.index_service = index_service or IndexCalculationServiceV2()
        self.gap_service = gap_service or GapAnalysisServiceV2()
        
        # è³‡æºç®¡ç†
        self.resource_pool = resource_pool or ResourcePoolManager()
        self.retry_strategy = AdaptiveRetryStrategy()
        
        # é…ç½®
        self.enable_partial_results = enable_partial_results
        
        # çµ±è¨ˆ
        self.stats = {
            "total_requests": 0,
            "partial_successes": 0,
            "complete_failures": 0,
            "avg_response_time": 0
        }
    
    async def analyze(
        self,
        resume: str,
        job_description: str,
        keywords: List[str],
        language: str = "en",
        analysis_options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œçµ„åˆåˆ†æï¼ˆç„¡å¿«å–ç‰ˆæœ¬ï¼‰ã€‚
        """
        
        self.stats["total_requests"] += 1
        start_time = time.time()
        
        try:
            # ç›´æ¥åŸ·è¡Œåˆ†æï¼Œä¸æª¢æŸ¥å¿«å–
            result = await self._execute_parallel_analysis(
                resume, job_description, keywords, language, analysis_options
            )
            
            # æ›´æ–°çµ±è¨ˆ
            elapsed = time.time() - start_time
            self._update_avg_response_time(elapsed)
            
            return result
                
            except Exception as e:
                if self.enable_partial_results:
                    # å˜—è©¦è¿”å›éƒ¨åˆ†çµæœ
                    return await self._handle_partial_failure(
                        e, resume, job_description, keywords
                    )
                raise
    
    async def _execute_parallel_analysis(
        self,
        resume: str,
        job_description: str,
        keywords: List[str],
        language: str,
        analysis_options: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å®Œå…¨ä¸¦è¡Œç­–ç•¥åŸ·è¡Œåˆ†æã€‚
        """
        
        # å¾è³‡æºæ± ç²å–å®¢æˆ¶ç«¯
        async with self.resource_pool.get_client() as client:
            # Phase 1: ä¸¦è¡Œç”Ÿæˆ embeddings
            embeddings = await self._generate_embeddings_parallel(
                client, resume, job_description
            )
            
            # Phase 2: ä¸¦è¡ŒåŸ·è¡Œ index è¨ˆç®—å’Œæº–å‚™ gap context
            async with asyncio.TaskGroup() as tg:
                # Index calculation with embeddings
                index_task = tg.create_task(
                    self.index_service.calculate_with_embeddings(
                        embeddings["resume"],
                        embeddings["job_description"],
                        keywords,
                        resume_text=resume,
                        job_text=job_description
                    )
                )
                
                # Prepare context for gap analysis
                context_task = tg.create_task(
                    self._prepare_gap_context(resume, job_description, language)
                )
        
        index_result = index_task.result()
        gap_context = context_task.result()
        
        # Phase 3: Gap analysis with index results
        with self.performance_monitor.track_operation("gap_analysis"):
            gap_result = await self._execute_gap_analysis_with_retry(
                resume,
                job_description,
                index_result,
                gap_context,
                analysis_options
            )
        
        # çµ„åˆçµæœ
        return {
            "index_calculation": index_result,
            "gap_analysis": gap_result,
            "metadata": {
                "language_detected": gap_context["language"],
                "cache_used": {
                    "embeddings": embeddings.get("from_cache", False),
                    "index": False,
                    "gap": False
                }
            }
        }
    
    async def _generate_embeddings_parallel(
        self,
        client: Any,
        resume: str,
        job_description: str
    ) -> Dict[str, Any]:
        """
        ä¸¦è¡Œç”Ÿæˆ embeddingsï¼ˆç„¡å¿«å–ï¼‰ã€‚
        """
        
        # ç›´æ¥ä¸¦è¡Œç”Ÿæˆï¼Œä¸æª¢æŸ¥å¿«å–
        async with asyncio.TaskGroup() as tg:
            resume_task = tg.create_task(
                self._create_embedding(client, resume)
            )
            job_task = tg.create_task(
                self._create_embedding(client, job_description)
            )
        
        return {
            "resume": resume_task.result(),
            "job_description": job_task.result()
        }
    
    async def _create_embedding(self, client: Any, text: str) -> List[float]:
        """
        ä½¿ç”¨æä¾›çš„å®¢æˆ¶ç«¯å‰µå»º embeddingã€‚
        """
        # ä½¿ç”¨è³‡æºæ± ä¸­çš„å®¢æˆ¶ç«¯ï¼Œé¿å…é‡è¤‡åˆå§‹åŒ–
        response = await client.embeddings.create(
            input=text,
            model="text-embedding-3-large"
        )
        return response.data[0].embedding
    
    async def _execute_gap_analysis_with_retry(
        self,
        resume: str,
        job_description: str,
        index_result: Dict[str, Any],
        context: Dict[str, Any],
        analysis_options: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œ gap analysis with æ™ºèƒ½é‡è©¦ã€‚
        """
        
        async def gap_analysis_with_context():
            return await self.gap_service.analyze_with_context(
                resume=resume,
                job_description=job_description,
                index_result=index_result,
                language=context["language"],
                options=analysis_options
            )
        
        # ä½¿ç”¨è‡ªé©æ‡‰é‡è©¦
        return await self.retry_strategy.execute_with_retry(
            gap_analysis_with_context,
            error_classifier=self._classify_gap_error
        )
    
    def _classify_gap_error(self, error: Exception) -> str:
        """
        åˆ†é¡éŒ¯èª¤é¡å‹ä»¥æ±ºå®šé‡è©¦ç­–ç•¥ã€‚
        """
        
        error_msg = str(error).lower()
        
        if "empty" in error_msg or "missing" in error_msg:
            return "empty_fields"
        elif "timeout" in error_msg:
            return "timeout"
        elif "rate" in error_msg and "limit" in error_msg:
            return "rate_limit"
        else:
            return "general"
    
    async def _handle_partial_failure(
        self,
        error: Exception,
        resume: str,
        job_description: str,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """
        è™•ç†éƒ¨åˆ†å¤±æ•—ï¼Œè¿”å›å¯ç”¨çµæœã€‚
        """
        
        self.stats["partial_successes"] += 1
        
        partial_result = {
            "index_calculation": None,
            "gap_analysis": None,
            "partial_failure": True,
            "error_details": {
                "message": str(error),
                "type": type(error).__name__
            }
        }
        
        # è‡³å°‘å˜—è©¦è¨ˆç®— index
        try:
            partial_result["index_calculation"] = await self.index_service.calculate_index(
                resume, job_description, keywords
            )
        except Exception as index_error:
            self.logger.error(f"Index calculation also failed: {index_error}")
            self.stats["complete_failures"] += 1
            raise
        
        return partial_result
    
    def get_service_stats(self) -> Dict[str, Any]:
        """
        ç²å–æœå‹™çµ±è¨ˆè³‡è¨Šã€‚
        """
        
        cache_stats = self.cache_manager.get_stats()
        perf_stats = self.performance_monitor.get_summary() if self.performance_monitor else {}
        
        return {
            "service": "CombinedAnalysisServiceV2",
            "requests": self.stats,
            "cache": cache_stats,
            "performance": perf_stats,
            "health": {
                "status": "healthy",
                "uptime_seconds": self._get_uptime(),
                "error_rate": self._calculate_error_rate()
            }
        }
```

### 3. è³‡æºæ± ç®¡ç†å™¨ï¼ˆå¯¦éš›å¯¦ä½œï¼‰

```python
# src/services/resource_pools/llm_resource_pool.py

from typing import Any, Optional
import asyncio
from contextlib import asynccontextmanager
import logging

from src.services.llm_factory import get_llm_client  # ğŸš¨ ä½¿ç”¨ LLM Factoryï¼

logger = logging.getLogger(__name__)

class ResourcePoolManager:
    """
    è³‡æºæ± ç®¡ç†å™¨ï¼Œæ¸›å°‘å®¢æˆ¶ç«¯åˆå§‹åŒ–é–‹éŠ·ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - é å»ºç«‹ OpenAI å®¢æˆ¶ç«¯æ± 
    - é€£ç·šé‡ç”¨
    - è‡ªå‹•æ“´å±•å’Œæ”¶ç¸®
    """
    
    def __init__(
        self,
        min_pool_size: int = 2,
        max_pool_size: int = 10,
        idle_timeout: int = 300  # 5 minutes
    ):
        self.settings = get_settings()
        self.min_pool_size = min_pool_size
        self.max_pool_size = max_pool_size
        self.idle_timeout = idle_timeout
        
        # å®¢æˆ¶ç«¯æ± 
        self.available_clients: asyncio.Queue = asyncio.Queue()
        self.active_clients: List[AsyncOpenAI] = []
        self.total_created = 0
        
        # é–å’Œç‹€æ…‹
        self.pool_lock = asyncio.Lock()
        self.initialized = False
        
        # çµ±è¨ˆ
        self.stats = {
            "clients_created": 0,
            "clients_reused": 0,
            "current_pool_size": 0,
            "peak_pool_size": 0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–è³‡æºæ± ã€‚"""
        if self.initialized:
            return
            
        async with self.pool_lock:
            if self.initialized:
                return
                
            # é å»ºç«‹æœ€å°æ•¸é‡çš„å®¢æˆ¶ç«¯
            for _ in range(self.min_pool_size):
                client = await self._create_client()
                await self.available_clients.put(client)
                
            self.initialized = True
            logger.info(f"Resource pool initialized with {self.min_pool_size} clients")
    
    async def _create_client(self) -> AsyncOpenAI:
        """å‰µå»ºæ–°çš„ OpenAI å®¢æˆ¶ç«¯ã€‚"""
        client = AsyncOpenAI(
            api_key=self.settings.azure_openai_api_key,
            base_url=f"{self.settings.azure_openai_endpoint}/openai/deployments/{self.settings.azure_openai_deployment}",
            default_headers={
                "api-key": self.settings.azure_openai_api_key
            },
            default_query={
                "api-version": self.settings.azure_openai_api_version
            }
        )
        
        self.total_created += 1
        self.stats["clients_created"] += 1
        self.stats["current_pool_size"] += 1
        self.stats["peak_pool_size"] = max(
            self.stats["peak_pool_size"],
            self.stats["current_pool_size"]
        )
        
        return client
    
    @asynccontextmanager
    async def get_client(self):
        """ç²å–å®¢æˆ¶ç«¯çš„ context managerã€‚"""
        if not self.initialized:
            await self.initialize()
            
        client = None
        try:
            # å˜—è©¦å¾æ± ä¸­ç²å–
            try:
                client = await asyncio.wait_for(
                    self.available_clients.get(),
                    timeout=0.1
                )
                self.stats["clients_reused"] += 1
            except asyncio.TimeoutError:
                # æ± ä¸­æ²’æœ‰å¯ç”¨å®¢æˆ¶ç«¯ï¼Œå‰µå»ºæ–°çš„
                if self.total_created < self.max_pool_size:
                    client = await self._create_client()
                else:
                    # é”åˆ°ä¸Šé™ï¼Œç­‰å¾…å¯ç”¨å®¢æˆ¶ç«¯
                    client = await self.available_clients.get()
                    self.stats["clients_reused"] += 1
            
            yield client
            
        finally:
            # æ­¸é‚„å®¢æˆ¶ç«¯åˆ°æ± ä¸­
            if client:
                await self.available_clients.put(client)
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–è³‡æºæ± çµ±è¨ˆã€‚"""
        return {
            "pool_stats": self.stats,
            "efficiency": {
                "reuse_rate": (
                    self.stats["clients_reused"] / 
                    (self.stats["clients_reused"] + self.stats["clients_created"])
                    if (self.stats["clients_reused"] + self.stats["clients_created"]) > 0
                    else 0
                ),
                "pool_utilization": (
                    (self.stats["current_pool_size"] - self.available_clients.qsize()) /
                    self.stats["current_pool_size"]
                    if self.stats["current_pool_size"] > 0
                    else 0
                )
            }
        }
```

### 4. è‡ªé©æ‡‰é‡è©¦ç­–ç•¥

```python
# src/utils/adaptive_retry.py

from typing import Callable, Any, Dict, Optional
import asyncio
import random
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class AdaptiveRetryStrategy:
    """
    è‡ªé©æ‡‰é‡è©¦ç­–ç•¥ï¼Œæ ¹æ“šéŒ¯èª¤é¡å‹å‹•æ…‹èª¿æ•´é‡è©¦è¡Œç‚ºã€‚
    """
    
    def __init__(self):
        # éŒ¯èª¤é¡å‹é…ç½®
        self.retry_configs = {
            "empty_fields": {
                "max_attempts": 2,
                "base_delay": 1.0,
                "max_delay": 3.0,
                "backoff": "linear",
                "jitter": True
            },
            "timeout": {
                "max_attempts": 3,
                "base_delay": 0.5,
                "max_delay": 5.0,
                "backoff": "exponential",
                "jitter": True
            },
            "rate_limit": {
                "max_attempts": 3,
                "base_delay": 5.0,
                "max_delay": 30.0,
                "backoff": "exponential",
                "jitter": False
            },
            "general": {
                "max_attempts": 3,
                "base_delay": 1.0,
                "max_delay": 10.0,
                "backoff": "exponential",
                "jitter": True
            }
        }
        
        # é‡è©¦çµ±è¨ˆ
        self.retry_stats = {
            error_type: {"attempts": 0, "successes": 0, "failures": 0}
            for error_type in self.retry_configs
        }
    
    async def execute_with_retry(
        self,
        func: Callable,
        error_classifier: Optional[Callable] = None,
        on_retry: Optional[Callable] = None
    ) -> Any:
        """
        åŸ·è¡Œå‡½æ•¸ä¸¦æ ¹æ“šéŒ¯èª¤é¡å‹é€²è¡Œæ™ºèƒ½é‡è©¦ã€‚
        
        Args:
            func: è¦åŸ·è¡Œçš„ç•°æ­¥å‡½æ•¸
            error_classifier: éŒ¯èª¤åˆ†é¡å‡½æ•¸
            on_retry: é‡è©¦æ™‚çš„å›èª¿å‡½æ•¸
            
        Returns:
            å‡½æ•¸åŸ·è¡Œçµæœ
        """
        
        last_error = None
        error_type = "general"
        
        for attempt in range(self._get_max_attempts()):
            try:
                # åŸ·è¡Œå‡½æ•¸
                result = await func()
                
                # æˆåŠŸï¼Œæ›´æ–°çµ±è¨ˆ
                if attempt > 0:
                    self.retry_stats[error_type]["successes"] += 1
                    logger.info(f"Retry succeeded on attempt {attempt + 1}")
                
                return result
                
            except Exception as e:
                last_error = e
                
                # åˆ†é¡éŒ¯èª¤
                if error_classifier:
                    error_type = error_classifier(e)
                else:
                    error_type = self._default_error_classifier(e)
                
                config = self.retry_configs.get(error_type, self.retry_configs["general"])
                
                # æª¢æŸ¥æ˜¯å¦é‚„èƒ½é‡è©¦
                if attempt >= config["max_attempts"] - 1:
                    self.retry_stats[error_type]["failures"] += 1
                    logger.error(f"All retry attempts failed for {error_type} error")
                    raise
                
                # è¨ˆç®—å»¶é²
                delay = self._calculate_delay(config, attempt)
                
                # è¨˜éŒ„é‡è©¦
                self.retry_stats[error_type]["attempts"] += 1
                logger.warning(
                    f"Attempt {attempt + 1} failed with {error_type} error. "
                    f"Retrying in {delay:.2f}s..."
                )
                
                # åŸ·è¡Œé‡è©¦å›èª¿
                if on_retry:
                    await on_retry(attempt, error_type, delay)
                
                # ç­‰å¾…
                await asyncio.sleep(delay)
        
        raise last_error
    
    def _calculate_delay(self, config: Dict[str, Any], attempt: int) -> float:
        """è¨ˆç®—é‡è©¦å»¶é²ã€‚"""
        
        base_delay = config["base_delay"]
        max_delay = config["max_delay"]
        backoff = config["backoff"]
        
        if backoff == "linear":
            delay = base_delay * (attempt + 1)
        elif backoff == "exponential":
            delay = base_delay * (2 ** attempt)
        else:
            delay = base_delay
        
        # é™åˆ¶æœ€å¤§å»¶é²
        delay = min(delay, max_delay)
        
        # æ·»åŠ æŠ–å‹•
        if config.get("jitter", False):
            jitter = random.uniform(0, delay * 0.1)
            delay += jitter
        
        return delay
    
    def _default_error_classifier(self, error: Exception) -> str:
        """é è¨­éŒ¯èª¤åˆ†é¡å™¨ã€‚"""
        
        error_msg = str(error).lower()
        
        if "timeout" in error_msg:
            return "timeout"
        elif "rate" in error_msg and "limit" in error_msg:
            return "rate_limit"
        elif "empty" in error_msg or "missing" in error_msg:
            return "empty_fields"
        else:
            return "general"
    
    def _get_max_attempts(self) -> int:
        """ç²å–æœ€å¤§é‡è©¦æ¬¡æ•¸ã€‚"""
        return max(config["max_attempts"] for config in self.retry_configs.values())
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–é‡è©¦çµ±è¨ˆã€‚"""
        
        total_stats = {
            "total_retries": sum(stats["attempts"] for stats in self.retry_stats.values()),
            "total_successes": sum(stats["successes"] for stats in self.retry_stats.values()),
            "total_failures": sum(stats["failures"] for stats in self.retry_stats.values()),
            "by_error_type": self.retry_stats
        }
        
        # è¨ˆç®—æˆåŠŸç‡
        if total_stats["total_retries"] > 0:
            total_stats["retry_success_rate"] = (
                total_stats["total_successes"] / total_stats["total_retries"]
            )
        else:
            total_stats["retry_success_rate"] = 0.0
        
        return total_stats
```

### 5. å‡ç´šç‰ˆ Gap Analysis Service

```python
# src/services/gap_analysis_v2.py

from typing import Dict, Any, List, Optional
import asyncio
from datetime import datetime

from src.services.gap_analysis import GapAnalysisService
from src.utils.prompt_manager import get_prompt

class GapAnalysisServiceV2(GapAnalysisService):
    """
    å‡ç´šç‰ˆ Gap Analysis æœå‹™ã€‚
    
    æ–°å¢åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ index çµæœä½œç‚ºä¸Šä¸‹æ–‡
    2. æ™ºèƒ½ prompt å„ªåŒ–
    3. ä¸²æµè™•ç†æ”¯æ´
    4. æŠ€èƒ½å„ªå…ˆç´šåˆ†æ
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.enable_context_enhancement = True
        self.enable_skill_priorities = True
    
    async def analyze_with_context(
        self,
        resume: str,
        job_description: str,
        index_result: Dict[str, Any],
        language: str = "en",
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ index çµæœä½œç‚ºä¸Šä¸‹æ–‡é€²è¡Œåˆ†æã€‚
        """
        
        # æ§‹å»ºå¢å¼·çš„ prompt
        enhanced_prompt = self._build_enhanced_prompt(
            resume,
            job_description,
            index_result,
            language,
            options
        )
        
        # åŸ·è¡Œåˆ†æ
        try:
            # å‘¼å« LLM
            response = await self._call_llm_with_context(
                enhanced_prompt,
                temperature=0.3,  # é™ä½æº«åº¦ä»¥æé«˜ä¸€è‡´æ€§
                max_tokens=1500
            )
            
            # è§£æçµæœ
            result = self._parse_enhanced_response(response)
            
            # æ·»åŠ æŠ€èƒ½å„ªå…ˆç´š
            if self.enable_skill_priorities and options and options.get("include_skill_priorities"):
                result["skill_priorities"] = await self._analyze_skill_priorities(
                    result["gaps"],
                    index_result
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Gap analysis with context failed: {e}")
            # é™ç´šåˆ°æ™®é€šåˆ†æ
            return await super().analyze_gap(resume, job_description, language)
    
    def _build_enhanced_prompt(
        self,
        resume: str,
        job_description: str,
        index_result: Dict[str, Any],
        language: str,
        options: Optional[Dict[str, Any]]
    ) -> str:
        """
        æ§‹å»ºå¢å¼·çš„ promptï¼ŒåŒ…å« index çµæœã€‚
        """
        
        # ç²å–åŸºç¤ prompt
        base_prompt = get_prompt("gap_analysis", version="2.0.0", language=language)
        
        # æ·»åŠ  index çµæœä¸Šä¸‹æ–‡
        context_section = f"""
### Analysis Context:
- Overall Match Score: {index_result.get('similarity_percentage', 0)}%
- Keyword Coverage: {index_result.get('keyword_coverage', {}).get('coverage_percentage', 0)}%
- Covered Keywords: {', '.join(index_result.get('keyword_coverage', {}).get('covered_keywords', [])[:10])}
- Missing Keywords: {', '.join(index_result.get('keyword_coverage', {}).get('missed_keywords', [])[:10])}

Please consider this matching data when providing your analysis.
"""
        
        # æ·»åŠ é¸é …æŒ‡ç¤º
        options_section = ""
        if options:
            if options.get("focus_areas"):
                areas = ", ".join(options["focus_areas"])
                options_section += f"\nFocus your analysis on: {areas}"
            
            if options.get("max_improvements"):
                options_section += f"\nLimit improvements to {options['max_improvements']} items"
        
        # çµ„åˆ prompt
        enhanced_prompt = base_prompt.format(
            job_description=job_description,
            resume=resume,
            context=context_section,
            options=options_section
        )
        
        return enhanced_prompt
    
    async def _analyze_skill_priorities(
        self,
        gaps: List[str],
        index_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        åˆ†ææŠ€èƒ½å„ªå…ˆç´šã€‚
        """
        
        missed_keywords = index_result.get("keyword_coverage", {}).get("missed_keywords", [])
        
        # ç°¡å–®çš„å„ªå…ˆç´šé‚è¼¯ï¼ˆå¯ä»¥æ›´è¤‡é›œï¼‰
        skill_priorities = []
        
        for keyword in missed_keywords[:5]:  # å‰5å€‹ç¼ºå¤±é—œéµå­—
            priority = "high" if keyword in gaps[0] if gaps else False else "medium"
            
            skill_priorities.append({
                "skill": keyword,
                "priority": priority,
                "reason": f"Required by job but not found in resume"
            })
        
        return skill_priorities
    
    def _parse_enhanced_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æå¢å¼·çš„å›æ‡‰ã€‚
        """
        
        # ä½¿ç”¨çˆ¶é¡çš„è§£æé‚è¼¯
        base_result = super()._parse_gap_response(response)
        
        # æ·»åŠ  V2 ç‰¹å®šæ¬„ä½
        base_result["version"] = "2.0"
        base_result["enhanced_analysis"] = True
        
        return base_result
```

## ğŸ“Š ç›£æ§é…ç½®ï¼ˆè¼•é‡ç´šç­–ç•¥ï¼‰

### è¼•é‡ç´šç›£æ§åŸå‰‡

éµå¾ª Keyword Extraction å’Œ Index Calculation V2 çš„æˆåŠŸç¶“é©—ï¼š

```python
# ç’°å¢ƒè®Šæ•¸é…ç½®
LIGHTWEIGHT_MONITORING=true  # ç”Ÿç”¢ç’°å¢ƒé è¨­
MONITORING_ENABLED=true      # å•Ÿç”¨ Application Insights

# åœ¨ API ç«¯é»ä¸­å¯¦ä½œ
async def calculate_index_and_analyze_gap(
    request: CombinedAnalysisRequest,
    api_key: str = Depends(verify_api_key)
) -> CombinedAnalysisResponse:
    start_time = time.time()
    
    try:
        # åŸ·è¡Œåˆ†æ
        result = await service.analyze(...)
        
        # è¼•é‡ç´šç›£æ§ - åªè¿½è¹¤é—œéµæŒ‡æ¨™
        if os.getenv('LIGHTWEIGHT_MONITORING', 'true').lower() == 'true':
            monitoring_service.track_event(
                "IndexCalAndGapAnalysisV2Completed",
                {
                    "success": True,
                    "processing_time_ms": round((time.time() - start_time) * 1000, 2),
                    "version": "v2",
                    "using_resource_pool": True
                }
            )
        else:
            # é–‹ç™¼ç’°å¢ƒçš„è©³ç´°ç›£æ§
            monitoring_service.track_event(
                "IndexCalAndGapAnalysisV2Detailed",
                {
                    "success": True,
                    "processing_time_ms": round((time.time() - start_time) * 1000, 2),
                    "timing_breakdown": result.get("timing_breakdown", {}),
                    "resource_pool_stats": resource_pool.get_stats(),
                    "version": "v2"
                }
            )
        
        return create_success_response(data=result)
        
    except Exception as e:
        # éŒ¯èª¤ç›£æ§ï¼ˆå§‹çµ‚å•Ÿç”¨ï¼‰
        monitoring_service.track_event(
            "IndexCalAndGapAnalysisV2Error",
            {
                "error": str(e),
                "error_type": type(e).__name__,
                "processing_time_ms": round((time.time() - start_time) * 1000, 2)
            }
        )
        raise
```

### Application Insights æ•´åˆï¼ˆç²¾ç°¡ç‰ˆï¼‰

```python
# src/utils/monitoring.py

from typing import Dict, Any, Optional
import time
from contextlib import contextmanager
from opencensus.ext.azure import metrics_exporter
from opencensus.stats import aggregation, measure, stats, view
from opencensus.tags import tag_map

class PerformanceMonitor:
    """å¢å¼·ç‰ˆæ•ˆèƒ½ç›£æ§ã€‚"""
    
    def __init__(self, enable_azure_insights: bool = True):
        self.enable_azure_insights = enable_azure_insights
        
        if enable_azure_insights:
            # è¨­å®š Azure ç›£æ§
            self._setup_azure_monitoring()
        
        # æœ¬åœ°æŒ‡æ¨™
        self.metrics = {}
    
    def _setup_azure_monitoring(self):
        """è¨­å®š Azure Application Insightsã€‚"""
        
        # å»ºç«‹é‡æ¸¬
        self.response_time_measure = measure.MeasureFloat(
            "response_time",
            "API response time in milliseconds",
            "ms"
        )
        
        self.cache_hit_measure = measure.MeasureInt(
            "cache_hit",
            "Cache hit count",
            "1"
        )
        
        # å»ºç«‹è¦–åœ–
        response_time_view = view.View(
            "combined_analysis_response_time",
            "Response time distribution",
            [],
            self.response_time_measure,
            aggregation.DistributionAggregation([50, 100, 200, 500, 1000, 2000, 5000])
        )
        
        cache_hit_view = view.View(
            "cache_hit_rate",
            "Cache hit rate",
            [],
            self.cache_hit_measure,
            aggregation.SumAggregation()
        )
        
        # è¨»å†Šè¦–åœ–
        stats.stats.view_manager.register_view(response_time_view)
        stats.stats.view_manager.register_view(cache_hit_view)
        
        # è¨­å®šåŒ¯å‡ºå™¨
        exporter = metrics_exporter.new_metrics_exporter(
            connection_string=os.environ.get("APPLICATIONINSIGHTS_CONNECTION_STRING")
        )
        
        stats.stats.view_manager.register_exporter(exporter)
    
    @contextmanager
    def track_operation(self, operation_name: str, tags: Optional[Dict[str, str]] = None):
        """è¿½è¹¤æ“ä½œæ•ˆèƒ½ã€‚"""
        
        start_time = time.time()
        
        try:
            yield
            status = "success"
        except Exception:
            status = "failure"
            raise
        finally:
            elapsed_ms = (time.time() - start_time) * 1000
            
            # è¨˜éŒ„æœ¬åœ°æŒ‡æ¨™
            if operation_name not in self.metrics:
                self.metrics[operation_name] = []
            
            self.metrics[operation_name].append({
                "elapsed_ms": elapsed_ms,
                "status": status,
                "timestamp": time.time()
            })
            
            # ç™¼é€åˆ° Azure
            if self.enable_azure_insights:
                tag_map_obj = tag_map.TagMap()
                tag_map_obj.insert("operation", operation_name)
                tag_map_obj.insert("status", status)
                
                if tags:
                    for key, value in tags.items():
                        tag_map_obj.insert(key, value)
                
                mmap = stats.stats.stats_recorder.new_measurement_map()
                mmap.measure_float_put(self.response_time_measure, elapsed_ms)
                mmap.record(tag_map_obj)
```

## ğŸ§ª æ¸¬è©¦å¯¦æ–½çµæœ

### æ¸¬è©¦çµ±è¨ˆ
- **ç¸½æ¸¬è©¦æ•¸**: 42 å€‹
- **é€šéç‡**: 100%
- **è¦†è“‹é¡å‹**: å–®å…ƒæ¸¬è©¦(20)ã€æ•´åˆæ¸¬è©¦(17)ã€æ•ˆèƒ½æ¸¬è©¦(2)ã€E2Eæ¸¬è©¦(3)

### é—œéµæ¸¬è©¦æ¡ˆä¾‹ï¼ˆå¯¦éš›å¯¦ä½œï¼‰

```python
# tests/unit/test_combined_analysis_v2.py

import pytest
from unittest.mock import Mock, AsyncMock, patch
import asyncio

from src.services.combined_analysis_v2 import CombinedAnalysisServiceV2
from src.services.shared_cache_manager import SharedCacheManager

class TestCombinedAnalysisServiceV2:
    """æ¸¬è©¦çµ„åˆåˆ†ææœå‹™ V2ã€‚"""
    
    @pytest.fixture
    def service(self):
        """å»ºç«‹æ¸¬è©¦æœå‹™å¯¦ä¾‹ã€‚"""
        index_service = Mock()
        gap_service = Mock()
        cache_manager = SharedCacheManager()
        
        return CombinedAnalysisServiceV2(
            index_service=index_service,
            gap_service=gap_service,
            cache_manager=cache_manager,
            enable_performance_monitoring=False
        )
    
    @pytest.mark.asyncio
    async def test_cache_hit_returns_immediately(self, service):
        """æ¸¬è©¦å¿«å–å‘½ä¸­æ™‚ç«‹å³è¿”å›ã€‚"""
        
        # è¨­å®šå¿«å–
        cached_result = {"cached": True, "data": "test"}
        cache_key = service._generate_cache_key("resume", "job", ["python"])
        await service.cache_manager.set_combined_result(cache_key, cached_result)
        
        # åŸ·è¡Œåˆ†æ
        result = await service.analyze("resume", "job", ["python"])
        
        # é©—è­‰
        assert result == cached_result
        assert service.stats["cache_hits"] == 1
        assert service.index_service.calculate_index.call_count == 0
    
    @pytest.mark.asyncio
    async def test_partial_failure_returns_index_only(self, service):
        """æ¸¬è©¦éƒ¨åˆ†å¤±æ•—æ™‚è¿”å› index çµæœã€‚"""
        
        # è¨­å®š mock
        service.index_service.calculate_index = AsyncMock(
            return_value={"similarity_percentage": 75}
        )
        service.gap_service.analyze_with_context = AsyncMock(
            side_effect=Exception("Gap analysis failed")
        )
        
        # åŸ·è¡Œåˆ†æ
        result = await service.analyze("resume", "job", ["python"])
        
        # é©—è­‰
        assert result["partial_failure"] is True
        assert result["index_calculation"] is not None
        assert result["gap_analysis"] is None
        assert service.stats["partial_successes"] == 1
    
    @pytest.mark.asyncio
    async def test_shared_embeddings_reduce_api_calls(self, service):
        """æ¸¬è©¦å…±äº« embeddings æ¸›å°‘ API å‘¼å«ã€‚"""
        
        with patch.object(service, '_create_embedding') as mock_create:
            mock_create.return_value = [0.1] * 1536
            
            # ç¬¬ä¸€æ¬¡å‘¼å«
            await service._generate_shared_embeddings("resume1", "job1")
            assert mock_create.call_count == 2
            
            # ç¬¬äºŒæ¬¡å‘¼å«ç›¸åŒæ–‡æœ¬
            await service._generate_shared_embeddings("resume1", "job1")
            assert mock_create.call_count == 2  # æ²’æœ‰æ–°çš„å‘¼å«
    
    @pytest.mark.asyncio
    async def test_concurrent_analysis_handling(self, service):
        """æ¸¬è©¦ä¸¦ç™¼åˆ†æè™•ç†ã€‚"""
        
        # è¨­å®š mock
        service.index_service.calculate_with_embeddings = AsyncMock(
            return_value={"similarity_percentage": 80}
        )
        service.gap_service.analyze_with_context = AsyncMock(
            return_value={"gaps": ["test gap"]}
        )
        
        # ä¸¦ç™¼åŸ·è¡Œå¤šå€‹åˆ†æ
        tasks = [
            service.analyze(f"resume{i}", f"job{i}", ["python"])
            for i in range(5)
        ]
        
        results = await asyncio.gather(*tasks)
        
        # é©—è­‰
        assert len(results) == 5
        assert all(r["index_calculation"] is not None for r in results)
        assert all(r["gap_analysis"] is not None for r in results)
```

### æ•´åˆæ¸¬è©¦ç¯„ä¾‹

```python
# tests/integration/test_full_analysis_flow.py

import pytest
import asyncio
from httpx import AsyncClient

from src.main import app

class TestFullAnalysisFlow:
    """æ¸¬è©¦å®Œæ•´åˆ†ææµç¨‹ã€‚"""
    
    @pytest.mark.asyncio
    async def test_v2_api_performance(self):
        """æ¸¬è©¦ V2 API æ•ˆèƒ½æ”¹é€²ã€‚"""
        
        async with AsyncClient(app=app, base_url="http://test") as client:
            # æº–å‚™æ¸¬è©¦è³‡æ–™
            request_data = {
                "resume": "Senior Python Developer with 10 years experience...",
                "job_description": "Looking for Python expert with FastAPI...",
                "keywords": ["Python", "FastAPI", "Docker", "AWS"]
            }
            
            # æ¸¬è©¦ V2
            import time
            start = time.time()
            
            response = await client.post(
                "/api/v1/index-cal-and-gap-analysis-v2",
                json=request_data,
                headers={"X-API-Key": "test-key"}
            )
            
            v2_time = time.time() - start
            
            # é©—è­‰
            assert response.status_code == 200
            data = response.json()
            assert data["success"] is True
            assert "index_calculation" in data["data"]
            assert "gap_analysis" in data["data"]
            assert v2_time < 4.0  # æ‡‰è©²åœ¨ 4 ç§’å…§å®Œæˆ
    
    @pytest.mark.asyncio
    async def test_cache_effectiveness(self):
        """æ¸¬è©¦å¿«å–æ•ˆæœã€‚"""
        
        async with AsyncClient(app=app, base_url="http://test") as client:
            request_data = {
                "resume": "Test resume content",
                "job_description": "Test job description",
                "keywords": ["test"]
            }
            
            # ç¬¬ä¸€æ¬¡è«‹æ±‚
            response1 = await client.post(
                "/api/v1/index-cal-and-gap-analysis-v2",
                json=request_data,
                headers={"X-API-Key": "test-key"}
            )
            
            time1 = response1.json()["data"]["processing_time_ms"]
            
            # ç¬¬äºŒæ¬¡è«‹æ±‚ï¼ˆæ‡‰è©²å‘½ä¸­å¿«å–ï¼‰
            response2 = await client.post(
                "/api/v1/index-cal-and-gap-analysis-v2",
                json=request_data,
                headers={"X-API-Key": "test-key"}
            )
            
            time2 = response2.json()["data"]["processing_time_ms"]
            
            # é©—è­‰å¿«å–æ•ˆæœ
            assert time2 < time1 * 0.1  # å¿«å–æ‡‰è©²å¿« 10 å€ä»¥ä¸Š
```

## ğŸš€ éƒ¨ç½²ç‹€æ…‹

### ç•¶å‰ç‹€æ…‹
- **é–‹ç™¼ç‹€æ…‹**: âœ… å®Œæˆ
- **æ¸¬è©¦ç‹€æ…‹**: âœ… å…¨éƒ¨é€šéï¼ˆ42/42ï¼‰
- **éƒ¨ç½²ç‹€æ…‹**: â³ å¾…éƒ¨ç½²
- **ç¨‹å¼ç¢¼å“è³ª**: âœ… Ruff æª¢æŸ¥é€šé

### ç’°å¢ƒè®Šæ•¸é…ç½®ï¼ˆå¯¦éš›éœ€è¦ï¼‰

```python
# src/utils/feature_flags.py

import os
from typing import Dict, Any

class FeatureFlags:
    """Feature flag ç®¡ç†ã€‚"""
    
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–
    USE_V2_IMPLEMENTATION = os.getenv("USE_V2_IMPLEMENTATION", "false").lower() == "true"
    
    # å¯é¸ï¼šç™¾åˆ†æ¯”æ§åˆ¶
    V2_ROLLOUT_PERCENTAGE = int(os.getenv("V2_ROLLOUT_PERCENTAGE", "0"))
    
    @classmethod
    def should_use_v2(cls, user_id: Optional[str] = None) -> bool:
        """æ±ºå®šæ˜¯å¦ä½¿ç”¨ V2 å¯¦ä½œã€‚"""
        if cls.USE_V2_IMPLEMENTATION:
            return True
            
        if cls.V2_ROLLOUT_PERCENTAGE > 0 and user_id:
            # åŸºæ–¼ user_id çš„ç©©å®šåˆ†é…
            return hash(user_id) % 100 < cls.V2_ROLLOUT_PERCENTAGE
            
        return False
```

### 2. æœ¬åœ°é–‹ç™¼å’Œæ¸¬è©¦

```bash
# æœ¬åœ°æ¸¬è©¦ V2 å¯¦ä½œ
export USE_V2_IMPLEMENTATION=true
python -m pytest tests/unit/test_combined_analysis_v2.py -v

# A/B æ•ˆèƒ½æ¯”è¼ƒ
python tests/performance/test_ab_comparison.py
```

### 3. å…§éƒ¨æ¸¬è©¦éƒ¨ç½²

```bash
# æ›´æ–°é–‹ç™¼ç’°å¢ƒçš„ feature flag
az containerapp update \
  --name airesumeadvisor-api-dev \
  --resource-group airesumeadvisorfastapi \
  --set-env-vars USE_V2_IMPLEMENTATION=true

# é©—è­‰ V2 å·²å•Ÿç”¨
curl https://airesumeadvisor-api-dev.calmisland-ea7fe91e.japaneast.azurecontainerapps.io/api/v1/debug/feature-flags

# åŸ·è¡Œå…§éƒ¨æ¸¬è©¦
python scripts/internal_testing.py --env dev
```

### 4. ç”Ÿç”¢ç’°å¢ƒåˆ‡æ›ï¼ˆç°¡åŒ–ç‰ˆï¼‰

```bash
# é€æ­¥å•Ÿç”¨ V2
# Step 1: 10% å…§éƒ¨æ¸¬è©¦
az containerapp update \
  --name airesumeadvisor-api-production \
  --resource-group airesumeadvisorfastapi \
  --set-env-vars V2_ROLLOUT_PERCENTAGE=10

# Step 2: ç›£æ§é—œéµæŒ‡æ¨™ï¼ˆç°¡åŒ–ç‰ˆï¼‰
python scripts/monitor_v2_simple.py --duration 1h

# Step 3: å…¨é¢å•Ÿç”¨
az containerapp update \
  --name airesumeadvisor-api-production \
  --resource-group airesumeadvisorfastapi \
  --set-env-vars USE_V2_IMPLEMENTATION=true
```

## ğŸ“ˆ ç°¡åŒ–ç›£æ§ï¼ˆå…§éƒ¨ä½¿ç”¨ï¼‰

### A/B æ•ˆèƒ½æ¯”è¼ƒè…³æœ¬

```python
# scripts/monitor_v2_simple.py

import asyncio
import statistics
from typing import List, Dict, Any

async def compare_v1_v2_performance(duration_hours: int = 1):
    """ç°¡å–®çš„ V1/V2 æ•ˆèƒ½æ¯”è¼ƒã€‚"""
    
    v1_times: List[float] = []
    v2_times: List[float] = []
    errors = {"v1": 0, "v2": 0}
    
    end_time = time.time() + (duration_hours * 3600)
    
    while time.time() < end_time:
        # æ¸¬è©¦æ¡ˆä¾‹
        test_case = generate_test_case()
        
        # V1 æ¸¬è©¦
        try:
            start = time.time()
            await test_with_flag(test_case, use_v2=False)
            v1_times.append(time.time() - start)
        except Exception:
            errors["v1"] += 1
            
        # V2 æ¸¬è©¦
        try:
            start = time.time()
            await test_with_flag(test_case, use_v2=True)
            v2_times.append(time.time() - start)
        except Exception:
            errors["v2"] += 1
            
        await asyncio.sleep(10)  # æ¯ 10 ç§’æ¸¬è©¦ä¸€æ¬¡
    
    # è¨ˆç®—çµæœ
    return {
        "v1": {
            "avg_time": statistics.mean(v1_times),
            "p95_time": statistics.quantiles(v1_times, n=20)[18],
            "error_rate": errors["v1"] / len(v1_times) if v1_times else 0
        },
        "v2": {
            "avg_time": statistics.mean(v2_times),
            "p95_time": statistics.quantiles(v2_times, n=20)[18],
            "error_rate": errors["v2"] / len(v2_times) if v2_times else 0
        },
        "improvement": {
            "avg_time": f"{(1 - statistics.mean(v2_times) / statistics.mean(v1_times)) * 100:.1f}%",
            "p95_time": f"{(1 - statistics.quantiles(v2_times, n=20)[18] / statistics.quantiles(v1_times, n=20)[18]) * 100:.1f}%"
        }
    }
```

### æ•ˆèƒ½æª¢æŸ¥æ¸…å–®ï¼ˆæ›´æ–°ç‰ˆï¼‰

- [ ] P95 éŸ¿æ‡‰æ™‚é–“ < 4 ç§’
- [ ] è³‡æºæ± é‡ç”¨ç‡ > 80%
- [ ] è¨˜æ†¶é«”ä½¿ç”¨ç©©å®šï¼ˆç„¡æ´©æ¼ï¼‰
- [ ] éŒ¯èª¤ç‡ < 1%
- [ ] ä¸¦è¡Œè™•ç†æå‡ > 40%

## ğŸ¯ æˆåŠŸæ¨™æº–èˆ‡å¯¦éš›é”æˆ

### 1. æ•ˆèƒ½æ”¹é€²ï¼ˆéƒ¨åˆ†é”æˆï¼‰
- **P50/P95 éŸ¿æ‡‰æ™‚é–“**: æœªé”åŸå§‹ç›®æ¨™ï¼Œä½†æ¯” V1 æ”¹å–„ 24-29%
- **è³‡æºæ± æ•ˆæœ**: âœ… 100% é‡ç”¨ç‡ï¼Œæœ‰æ•ˆæ¸›å°‘åˆå§‹åŒ–é–‹éŠ·
- **LLM Factory çµ±ä¸€**: âœ… é¿å…éƒ¨ç½²éŒ¯èª¤ï¼Œæå‡ç©©å®šæ€§

### 2. åŠŸèƒ½å®Œæ•´ï¼ˆå…¨éƒ¨é”æˆï¼‰
- **å‘å¾Œç›¸å®¹æ€§**: âœ… 100% API ç›¸å®¹
- **è¼¸å…¥é©—è­‰**: âœ… å®Œæ•´å¯¦ä½œï¼ˆæœ€å°é•·åº¦ã€èªè¨€ç™½åå–®ï¼‰
- **éŒ¯èª¤è™•ç†**: âœ… çµ±ä¸€æ ¼å¼ã€å®Œæ•´è¦†è“‹

### 3. æ¸¬è©¦èˆ‡å“è³ªï¼ˆè¶…è¶Šç›®æ¨™ï¼‰
- **æ¸¬è©¦è¦†è“‹**: âœ… 42 å€‹æ¸¬è©¦ 100% é€šé
- **ç¨‹å¼ç¢¼å“è³ª**: âœ… Ruff æª¢æŸ¥ç„¡éŒ¯èª¤
- **æ–‡æª”å®Œæ•´**: âœ… æŠ€è¡“æ–‡æª”ã€å¯¦ä½œæŒ‡å—ã€æ¸¬è©¦çŸ©é™£å®Œæ•´

## ğŸ“ å¯¦ä½œç¸½çµèˆ‡ç¶“é©—æ•™è¨“

### å·²å®Œæˆé …ç›®
1. **æ ¸å¿ƒåŠŸèƒ½** âœ…
   - çµ±ä¸€ LLM Factory ç®¡ç†ï¼ˆæœ€é‡è¦ï¼‰
   - è³‡æºæ± å¯¦ç¾ï¼ˆLLM + Embeddingï¼‰
   - å®Œæ•´è¼¸å…¥é©—è­‰ï¼ˆPydanticï¼‰
   - çµ±ä¸€éŒ¯èª¤è™•ç†

2. **æ¸¬è©¦å®Œæ•´** âœ…
   - 42 å€‹æ¸¬è©¦ 100% é€šé
   - å–®å…ƒã€æ•´åˆã€æ•ˆèƒ½ã€E2E å…¨è¦†è“‹
   - æ¸¬è©¦ç’°å¢ƒå®Œå…¨éš”é›¢

3. **æ–‡æª”é½Šå…¨** âœ…
   - æŠ€è¡“æ–‡æª”æ›´æ–°è‡³ v1.1.0
   - å¯¦ä½œæŒ‡å—åæ˜ å¯¦éš›å¯¦ä½œ
   - æ¸¬è©¦çŸ©é™£å®Œæ•´è¨˜éŒ„

### é—œéµç¶“é©—æ•™è¨“

1. **ğŸš¨ LLM Factory æ˜¯æ ¸å¿ƒ**
   - æœ¬æ¬¡è€—æ™‚æœ€å¤šçš„å•é¡Œå°±æ˜¯ Claude Code é•å LLM Factory è¦ç¯„
   - 9 å€‹æœå‹™ç›´æ¥ä½¿ç”¨ OpenAI SDK å°è‡´ 500 éŒ¯èª¤
   - å¿…é ˆåœ¨ CLAUDE.local.md å¼·èª¿æ­¤è¦å‰‡

2. **æ¸¬è©¦ç­–ç•¥æˆåŠŸ**
   - æ¸¬è©¦é©…å‹•é–‹ç™¼ç¢ºä¿å“è³ª
   - Mock ç­–ç•¥åˆ†å±¤è¨­è¨ˆæœ‰æ•ˆ
   - E2E ç¨ç«‹åŸ·è¡Œç’°å¢ƒå¿…è¦

3. **æ•ˆèƒ½å„ªåŒ–ç©ºé–“**
   - éŸ¿æ‡‰æ™‚é–“æœªé”åŸå§‹ç›®æ¨™ï¼ˆ< 2sï¼‰
   - ä¸»è¦å—é™æ–¼ Azure OpenAI API å»¶é²
   - è³‡æºæ± ç®¡ç†æœ‰æ•ˆä½†ä¸è¶³ä»¥é”æˆæ¿€é€²ç›®æ¨™

### å¾…éƒ¨ç½²é …ç›®
1. æ›´æ–° Container Apps ç’°å¢ƒè®Šæ•¸
2. è¨­å®š USE_V2_IMPLEMENTATION=true
3. ç›£æ§éƒ¨ç½²å¾Œæ•ˆèƒ½æŒ‡æ¨™
4. æº–å‚™å›æ»¾è¨ˆåŠƒ

---

**æ–‡æª”çµæŸ**
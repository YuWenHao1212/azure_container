# æ¸¬è©¦ç–‘é›£æ’è§£æŒ‡å—

**æ–‡æª”ç‰ˆæœ¬**: 1.0.0  
**å»ºç«‹æ—¥æœŸ**: 2025-08-19  
**ç¶­è­·è€…**: æ¸¬è©¦åœ˜éšŠ  
**ç›®çš„**: å¿«é€Ÿè¨ºæ–·å’Œè§£æ±ºæ¸¬è©¦å•é¡Œ

---

## ğŸ“š ç›®éŒ„

1. [å¸¸è¦‹å•é¡Œé€ŸæŸ¥è¡¨](#å¸¸è¦‹å•é¡Œé€ŸæŸ¥è¡¨)
2. [æŒ‰ç—‡ç‹€åˆ†é¡çš„è§£æ±ºæ–¹æ¡ˆ](#æŒ‰ç—‡ç‹€åˆ†é¡çš„è§£æ±ºæ–¹æ¡ˆ)
3. [è¨ºæ–·å·¥å…·å’ŒæŠ€å·§](#è¨ºæ–·å·¥å…·å’ŒæŠ€å·§)
4. [æ­·å²æ¡ˆä¾‹åˆ†æ](#æ­·å²æ¡ˆä¾‹åˆ†æ)
5. [é é˜²æªæ–½](#é é˜²æªæ–½)

---

## ğŸ” å¸¸è¦‹å•é¡Œé€ŸæŸ¥è¡¨

| å•é¡Œé¡å‹ | å‡ºç¾é »ç‡ | åš´é‡ç¨‹åº¦ | å…¸å‹ç—‡ç‹€ | å¿«é€Ÿè§£æ³• |
|---------|----------|----------|---------|----------|
| **LLM Factory é•è¦** | â­â­â­â­â­ | ğŸ”´ é«˜ | `deployment does not exist` | ä½¿ç”¨ `get_llm_client()` |
| **CI ç’°å¢ƒå¤±æ•—** | â­â­â­â­ | ğŸ”´ é«˜ | æœ¬åœ°é CI æ› | æ¢ä»¶åˆå§‹åŒ– |
| **Mock é¡å‹éŒ¯èª¤** | â­â­â­ | ğŸŸ¡ ä¸­ | `coroutine raised StopIteration` | ä½¿ç”¨ AsyncMock |
| **è³‡æ–™é•·åº¦ä¸è¶³** | â­â­â­ | ğŸŸ¡ ä¸­ | HTTP 422 éŒ¯èª¤ | ç¢ºä¿ â‰¥200 å­—å…ƒ |
| **JSON è§£æå¤±æ•—** | â­â­ | ğŸŸ¡ ä¸­ | `JSONDecodeError` | å¢åŠ  max_tokens |
| **æ¸¬è©¦äº’ç›¸å½±éŸ¿** | â­â­ | ğŸŸ¢ ä½ | åŸ·è¡Œé †åºæ•æ„Ÿ | fixture éš”é›¢ |

---

## ğŸ”§ æŒ‰ç—‡ç‹€åˆ†é¡çš„è§£æ±ºæ–¹æ¡ˆ

### ç—‡ç‹€ 1: "deployment does not exist" (500 éŒ¯èª¤)

#### éŒ¯èª¤è¨Šæ¯
```
Error: The deployment 'gpt-4o-2' does not exist
Status Code: 500 Internal Server Error
```

#### æ ¹æœ¬åŸå› 
ç›´æ¥ä½¿ç”¨ OpenAI SDK è€Œé LLM Factoryï¼Œå°è‡´éƒ¨ç½²åç¨±æ˜ å°„å¤±æ•—

#### è¨ºæ–·æ­¥é©Ÿ
```bash
# 1. æœå°‹é•è¦ä»£ç¢¼
grep -r "get_azure_openai_client\|AsyncAzureOpenAI" src/ test/

# 2. æª¢æŸ¥ import
grep -r "from openai import\|from src.services.openai_client" src/

# 3. æŸ¥çœ‹éŒ¯èª¤å †ç–Š
pytest test/failing_test.py -xvs --tb=long
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âŒ éŒ¯èª¤ä»£ç¢¼
from openai import AsyncAzureOpenAI
client = AsyncAzureOpenAI(...)

from src.services.openai_client import get_azure_openai_client
client = get_azure_openai_client()

# âœ… æ­£ç¢ºä»£ç¢¼
from src.services.llm_factory import get_llm_client
client = get_llm_client(api_name="gap_analysis")
```

#### é é˜²æªæ–½
- Code review æ™‚æœå°‹ OpenAI ç›´æ¥èª¿ç”¨
- åœ¨ CLAUDE.local.md æ˜ç¢ºæ¨™è¨»æ­¤è¦å‰‡
- ä½¿ç”¨ pre-commit hook æª¢æŸ¥

---

### ç—‡ç‹€ 2: "æ¸¬è©¦åœ¨æœ¬åœ°é€šéä½†åœ¨ CI å¤±æ•—"

#### éŒ¯èª¤è¨Šæ¯
```
Database configuration not found. Please set POSTGRES_* environment variables
FileNotFoundError: config/postgres_connection.json
```

#### æ ¹æœ¬åŸå› 
CI ç’°å¢ƒæ²’æœ‰æœ¬åœ°é…ç½®æª”æ¡ˆæˆ–ç’°å¢ƒè®Šæ•¸

#### è¨ºæ–·æ­¥é©Ÿ
```bash
# 1. æ¨¡æ“¬ CI ç’°å¢ƒ
CI=true GITHUB_ACTIONS=true pytest test/failing_test.py -xvs

# 2. æª¢æŸ¥é…ç½®ä¾è³´
find . -name "*.json" -o -name "*.env" | grep -E "(config|env)"

# 3. é‹è¡Œè¨ºæ–·è…³æœ¬
python test/scripts/diagnose_ci_env.py
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âŒ éŒ¯èª¤ï¼šç„¡æ¢ä»¶åˆå§‹åŒ–
async def get_data(self):
    await self.initialize()  # ç¸½æ˜¯å˜—è©¦é€£ç·š
    return await self._fetch()

# âœ… æ­£ç¢ºï¼šæ¢ä»¶åˆå§‹åŒ–
async def get_data(self):
    if not self._connection_pool:  # åªåœ¨éœ€è¦æ™‚åˆå§‹åŒ–
        await self.initialize()
    return await self._fetch()
```

#### CI ç’°å¢ƒé…ç½®
```python
# test/conftest.py
@pytest.fixture(autouse=True)
def mock_external_dependencies():
    """CI ç’°å¢ƒè‡ªå‹• mock å¤–éƒ¨ä¾è³´"""
    if os.environ.get('CI') == 'true':
        with patch('src.services.database.connect'):
            yield
    else:
        yield
```

---

### ç—‡ç‹€ 3: "coroutine raised StopIteration"

#### éŒ¯èª¤è¨Šæ¯
```
RuntimeError: coroutine raised StopIteration
TypeError: object Mock can't be used in 'await' expression
```

#### æ ¹æœ¬åŸå› 
å°éåŒæ­¥å‡½æ•¸ä½¿ç”¨ Mock è€Œé AsyncMock

#### è¨ºæ–·æ­¥é©Ÿ
```python
# æª¢æŸ¥å‡½æ•¸æ˜¯å¦ç‚ºéåŒæ­¥
import asyncio
print(asyncio.iscoroutinefunction(target_function))

# æª¢æŸ¥ Mock é¡å‹
print(type(mock_object))
print(callable(mock_object))
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âŒ éŒ¯èª¤ï¼šåŒæ­¥ Mock ç”¨æ–¼éåŒæ­¥
mock_service.process = Mock(return_value=result)

# âœ… æ­£ç¢ºï¼šAsyncMock ç”¨æ–¼éåŒæ­¥
mock_service.process = AsyncMock(return_value=result)

# âœ… æ­£ç¢ºï¼šAsync Context Manager
class AsyncContextManager:
    def __init__(self, value):
        self.value = value
    async def __aenter__(self):
        return self.value
    async def __aexit__(self, *args):
        return None

mock_pool.acquire = lambda: AsyncContextManager(mock_conn)
```

---

### ç—‡ç‹€ 4: "Job description must be at least 200 characters"

#### éŒ¯èª¤è¨Šæ¯
```
HTTP 422 Unprocessable Entity
{
    "error": {
        "code": "VALIDATION_ERROR",
        "message": "Job description must be at least 200 characters"
    }
}
```

#### æ ¹æœ¬åŸå› 
æ¸¬è©¦è³‡æ–™ä¸ç¬¦åˆ API æœ€å°é•·åº¦è¦æ±‚

#### è¨ºæ–·æ­¥é©Ÿ
```python
# æª¢æŸ¥è³‡æ–™é•·åº¦
print(f"JD length: {len(job_description)}")
print(f"Resume length: {len(resume)}")

# é©—è­‰æ‰€æœ‰æ¸¬è©¦è³‡æ–™
for name, data in test_data.items():
    if len(data) < 200:
        print(f"âŒ {name}: {len(data)} chars")
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âŒ éŒ¯èª¤ï¼šå¤ªçŸ­çš„æ¸¬è©¦è³‡æ–™
test_jd = "Python Developer with FastAPI"  # 30 å­—å…ƒ

# âœ… æ­£ç¢ºï¼šä½¿ç”¨ fixture æä¾›å……è¶³è³‡æ–™
@pytest.fixture
def valid_jd():
    return """
    Senior Python Developer with 5+ years experience in FastAPI, 
    Django, Docker, Kubernetes, AWS. Strong background in 
    microservices, RESTful APIs, PostgreSQL, MongoDB, Redis. 
    Excellent problem-solving skills, team collaboration, and 
    ability to work in fast-paced agile environment.
    """.strip()  # 300+ å­—å…ƒ
```

---

### ç—‡ç‹€ 5: "JSONDecodeError: Expecting property name"

#### éŒ¯èª¤è¨Šæ¯
```
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes
Response preview: '{"CoreStrengths": ["Python", "FastAPI"'  # æ˜é¡¯æˆªæ–·
```

#### æ ¹æœ¬åŸå› 
LLM è¼¸å‡ºè¢« token é™åˆ¶æˆªæ–·

#### è¨ºæ–·æ­¥é©Ÿ
```python
# 1. æª¢æŸ¥å›æ‡‰å®Œæ•´æ€§
print(f"Response length: {len(response)}")
print(f"Last 50 chars: {response[-50:]}")

# 2. é©—è­‰ JSON çµæ§‹
def check_json_complete(s):
    return s.count('{') == s.count('}') and \
           s.count('[') == s.count(']')
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âœ… å‹•æ…‹è¨ˆç®— tokens
def calculate_tokens(resume, jd):
    chars_per_token = 3
    input_tokens = (len(resume) + len(jd)) // chars_per_token
    output_buffer = 2500  # åŸºæœ¬è¼¸å‡ºéœ€æ±‚
    
    if input_tokens > 2000:
        output_buffer += 1000  # å¤§è¼¸å…¥éœ€è¦æ›´å¤šè¼¸å‡ºç©ºé–“
    
    return input_tokens + output_buffer

# âœ… JSON ä¿®å¾©æ©Ÿåˆ¶
def repair_truncated_json(json_str):
    # è£œå……ç¼ºå¤±æ‹¬è™Ÿ
    open_braces = json_str.count('{')
    close_braces = json_str.count('}')
    json_str += '}' * (open_braces - close_braces)
    
    # ç§»é™¤å°¾éš¨é€—è™Ÿ
    json_str = re.sub(r',\s*}', '}', json_str)
    return json_str
```

---

### ç—‡ç‹€ 6: "æ¸¬è©¦åŸ·è¡Œé †åºå½±éŸ¿çµæœ"

#### éŒ¯èª¤è¨Šæ¯
```
PASSED when run alone
FAILED when run with other tests
Import errors after certain test runs
```

#### æ ¹æœ¬åŸå› 
- å…¨å±€ç‹€æ…‹æ±¡æŸ“
- sys.path ä¿®æ”¹
- å–®ä¾‹æœªé‡ç½®

#### è¨ºæ–·æ­¥é©Ÿ
```bash
# 1. å–®ç¨åŸ·è¡Œæ¸¬è©¦
pytest test/test_file.py::test_function -xvs

# 2. éš¨æ©Ÿé †åºåŸ·è¡Œ
pytest --random-order

# 3. åå‘åŸ·è¡Œ
pytest --reverse
```

#### è§£æ±ºæ–¹æ¡ˆ
```python
# âœ… ä½¿ç”¨ fixture éš”é›¢
@pytest.fixture(autouse=True)
def reset_singleton():
    """é‡ç½®å–®ä¾‹ç‹€æ…‹"""
    MyService._instance = None
    yield
    MyService._instance = None

# âœ… ç’°å¢ƒè®Šæ•¸éš”é›¢
@pytest.fixture
def isolated_env():
    original = os.environ.copy()
    yield
    os.environ.clear()
    os.environ.update(original)

# âŒ é¿å…ä¿®æ”¹ sys.path
# sys.path.insert(0, path)  # ä¸è¦é€™æ¨£åš
```

---

## ğŸ› ï¸ è¨ºæ–·å·¥å…·å’ŒæŠ€å·§

### åŸºæœ¬è¨ºæ–·å‘½ä»¤

```bash
# é¡¯ç¤ºå¤±æ•—æ¸¬è©¦è©³æƒ…
pytest --lf -xvs  # åªè·‘ä¸Šæ¬¡å¤±æ•—çš„

# é€²å…¥é™¤éŒ¯æ¨¡å¼
pytest test.py --pdb  # å¤±æ•—æ™‚é€²å…¥ pdb

# é¡¯ç¤ºæ‰€æœ‰ print è¼¸å‡º
pytest -s

# é¡¯ç¤ºè©³ç´°å †ç–Š
pytest --tb=long

# æ¸¬è©¦åŸ·è¡Œæ™‚é–“åˆ†æ
pytest --durations=10  # é¡¯ç¤ºæœ€æ…¢çš„ 10 å€‹æ¸¬è©¦
```

### é€²éšè¨ºæ–·æŠ€å·§

#### 1. Mock èª¿ç”¨è¿½è¹¤
```python
# æŸ¥çœ‹æ‰€æœ‰ mock èª¿ç”¨
print(mock_object.mock_calls)
print(mock_object.call_args_list)

# é©—è­‰ç‰¹å®šèª¿ç”¨
mock_object.assert_called_with(expected_args)
mock_object.assert_called_once()
```

#### 2. ç’°å¢ƒè®Šæ•¸æª¢æŸ¥
```python
# è¨ºæ–· fixture
@pytest.fixture(autouse=True)
def log_environment():
    print("\n=== Test Environment ===")
    print(f"CI: {os.environ.get('CI')}")
    print(f"GITHUB_ACTIONS: {os.environ.get('GITHUB_ACTIONS')}")
    print(f"USE_V2: {os.environ.get('USE_V2_IMPLEMENTATION')}")
    yield
```

#### 3. æ¸¬è©¦éš”é›¢é©—è­‰
```bash
# åŸ·è¡Œå–®ä¸€æ¸¬è©¦å¤šæ¬¡
pytest test.py::test_func -count=5

# ä¸åŒé †åºåŸ·è¡Œ
pytest --random-order-seed=1234
pytest --random-order-seed=5678
```

---

## ğŸ“– æ­·å²æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹ 1: Course Batch Query CI å¤±æ•—ï¼ˆ2025-08-19ï¼‰

**æ™‚é–“ç·š**:
- Commit 386ff72: âœ… æ¸¬è©¦é€šé
- Commit 13f6bba: âŒ æ–°å¢ HTML åŠŸèƒ½å¾Œå¤±æ•—
- è¨ºæ–·æ™‚é–“: 2 å°æ™‚
- æ ¹æœ¬åŸå› : ç„¡æ¢ä»¶ `await self.initialize()`

**æ•™è¨“**: é‡æ§‹æ™‚ä¿ç•™é˜²ç¦¦æ€§æª¢æŸ¥

### æ¡ˆä¾‹ 2: LLM Factory é•è¦ï¼ˆ2025-08-05ï¼‰

**å•é¡Œ**: 9 å€‹æœå‹™ç›´æ¥ä½¿ç”¨ OpenAI SDK
**å½±éŸ¿**: æ‰€æœ‰æ•ˆèƒ½æ¸¬è©¦å¤±æ•—
**ä¿®å¾©æ™‚é–“**: 8 å°æ™‚ï¼ˆå…¶ä¸­ 2 å°æ™‚åœ¨æ­¤å•é¡Œï¼‰

**æ•™è¨“**: Claude Code éœ€è¦æ˜ç¢ºå‘ŠçŸ¥å°ˆæ¡ˆè¦ç¯„

### æ¡ˆä¾‹ 3: Pre-commit Hook å¤±æ•—ï¼ˆ2025-08-09ï¼‰

**å•é¡Œ**: AsyncMock å¯¦ä½œä¸å®Œæ•´
**ç—‡ç‹€**: `coroutine raised StopIteration`
**è§£æ±º**: å¯¦ä½œå®Œæ•´ async context manager

**æ•™è¨“**: æ¸¬è©¦ async å”è­°éœ€è¦å®Œæ•´å¯¦ä½œ

---

## ğŸ›¡ï¸ é é˜²æªæ–½

### é–‹ç™¼éšæ®µé é˜²

1. **ä½¿ç”¨æ¨¡æ¿å’Œ Snippets**
```python
# VS Code snippet for test
"pytest-async": {
    "prefix": "pytest-async",
    "body": [
        "@pytest.mark.asyncio",
        "async def test_${1:name}():",
        "    \"\"\"",
        "    Test ID: API-${2:MODULE}-${3:001}-UT",
        "    ${4:description}",
        "    \"\"\"",
        "    ${0:pass}"
    ]
}
```

2. **Pre-commit Hooks**
```yaml
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: check-llm-factory
      name: Check LLM Factory Usage
      entry: ./scripts/check_llm_factory.sh
      language: script
      files: \.(py)$
```

3. **è‡ªå‹•åŒ–æª¢æŸ¥è…³æœ¬**
```bash
#!/bin/bash
# scripts/check_llm_factory.sh
if grep -r "AsyncAzureOpenAI\|get_azure_openai_client" src/ test/; then
    echo "âŒ Found direct OpenAI SDK usage!"
    exit 1
fi
```

### Code Review æª¢æŸ¥é»

- [ ] ç„¡ç›´æ¥ OpenAI SDK import
- [ ] æ¸¬è©¦è³‡æ–™ä½¿ç”¨ fixtures
- [ ] Mock é¡å‹æ­£ç¢ºï¼ˆAsync/Syncï¼‰
- [ ] ç’°å¢ƒè®Šæ•¸åœ¨ fixture ç®¡ç†
- [ ] è³‡æºåˆå§‹åŒ–æœ‰æ¢ä»¶åˆ¤æ–·
- [ ] æ¸¬è©¦æœ‰ Test ID æ¨™è¨˜

### CI/CD é…ç½®å»ºè­°

```yaml
# .github/workflows/ci-cd-main.yml
- name: Run Tests with Diagnostics
  run: |
    # å•Ÿç”¨è¨ºæ–·æ¨¡å¼
    export TEST_DIAGNOSTICS=true
    export PYTEST_VERBOSE=true
    
    # åŸ·è¡Œæ¸¬è©¦ä¸¦ä¿å­˜è¼¸å‡º
    python test/scripts/pre_commit_check_advanced.py | tee test_output.log
    TEST_EXIT_CODE=${PIPESTATUS[0]}
    
    # å¤±æ•—æ™‚ä¸Šå‚³è¨ºæ–·è³‡è¨Š
    if [ $TEST_EXIT_CODE -ne 0 ]; then
        echo "::error::Tests failed"
        python test/scripts/diagnose_ci_env.py
        exit $TEST_EXIT_CODE
    fi
```

---

## ğŸ“Š å•é¡Œçµ±è¨ˆå’Œè¶¨å‹¢

### 2025 Q3 å•é¡Œåˆ†å¸ƒ
```
LLM Factory é•è¦:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35%
CI ç’°å¢ƒå•é¡Œ:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 25%
Mock é¡å‹éŒ¯èª¤:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%
è³‡æ–™é•·åº¦å•é¡Œ:        â–ˆâ–ˆâ–ˆâ–ˆ 15%
å…¶ä»–:               â–ˆâ–ˆ 5%
```

### æ”¹å–„è¶¨å‹¢
- LLM Factory é•è¦ï¼šå¾æ¯é€± 5 æ¬¡é™è‡³ 1 æ¬¡
- CI å¤±æ•—ç‡ï¼šå¾ 30% é™è‡³ 5%
- å¹³å‡ä¿®å¾©æ™‚é–“ï¼šå¾ 4 å°æ™‚é™è‡³ 30 åˆ†é˜

---

## ğŸš‘ ç·Šæ€¥è¯çµ¡å’Œè³‡æº

### å¿«é€Ÿä¿®å¾©è³‡æº
- [ç¶œåˆæ¸¬è©¦æŒ‡å—](./COMPREHENSIVE_TEST_DESIGN_GUIDE.md)
- [å¿«é€Ÿåƒè€ƒå¡](./TEST_QUICK_REFERENCE.md)
- [Mock ç­–ç•¥æŒ‡å—](../issues/service-module-refactor/MOCK_STRATEGY_GUIDE.md)

### è¨ºæ–·è…³æœ¬ä½ç½®
- `/test/scripts/diagnose_ci_env.py`
- `/test/scripts/check_mock_usage.py`
- `/test/scripts/validate_test_data.py`

### é—œéµé…ç½®æª”æ¡ˆ
- `/test/config/test_config.py`
- `/test/conftest.py`
- `/.github/workflows/ci-cd-main.yml`

---

**ç¶­è­·èªªæ˜**: æ¯æ¬¡é‡åˆ°æ–°å•é¡Œï¼Œæ›´æ–°æ­¤æ–‡æª”çš„æ¡ˆä¾‹åˆ†æç« ç¯€

**æœ€å¾Œæ›´æ–°**: 2025-08-19
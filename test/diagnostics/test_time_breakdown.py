#!/usr/bin/env python
"""
è¨ºæ–·è…³æœ¬:è©³ç´°åˆ†è§£ Course Availability æŸ¥è©¢çš„æ™‚é–“æ¶ˆè€—
ç›®çš„:æ‰¾å‡ºæ¯å€‹éšæ®µçš„è€—æ™‚,ç†è§£æ•ˆèƒ½ç“¶é ¸
"""
import asyncio
import json
import logging
import os
import sys
import time
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TimeTracker:
    """è¿½è¹¤å„éšæ®µæ™‚é–“çš„å·¥å…·"""

    def __init__(self):
        self.stages = []
        self.start_time = None

    def start(self, stage_name: str):
        """é–‹å§‹ä¸€å€‹éšæ®µ"""
        self.start_time = time.perf_counter()
        return self

    def end(self, stage_name: str) -> float:
        """çµæŸä¸€å€‹éšæ®µä¸¦è¨˜éŒ„æ™‚é–“"""
        if self.start_time is None:
            return 0
        elapsed = (time.perf_counter() - self.start_time) * 1000  # è½‰æ›ç‚º ms
        self.stages.append({
            "stage": stage_name,
            "duration_ms": round(elapsed, 2)
        })
        self.start_time = None
        return elapsed

    def get_breakdown(self) -> dict[str, Any]:
        """ç²å–æ™‚é–“åˆ†è§£å ±å‘Š"""
        total = sum(s["duration_ms"] for s in self.stages)
        return {
            "stages": self.stages,
            "total_ms": round(total, 2)
        }


async def analyze_single_query(iteration: int, use_cached_skills: bool = False) -> dict[str, Any]:
    """åˆ†æå–®æ¬¡æŸ¥è©¢çš„æ™‚é–“æ¶ˆè€—"""
    from src.services.course_availability import CourseAvailabilityChecker
    from src.services.course_search_singleton import get_course_search_service

    tracker = TimeTracker()
    result = {
        "iteration": iteration,
        "timestamp": datetime.now(UTC).isoformat(),
        "use_cached_skills": use_cached_skills
    }

    # æº–å‚™æ¸¬è©¦æŠ€èƒ½
    if use_cached_skills:
        # ä½¿ç”¨å¯èƒ½è¢«å¿«å–çš„ç†±é–€æŠ€èƒ½
        test_skills = [
            {"skill_name": "Python", "skill_category": "SKILL", "description": "Programming language"},
            {"skill_name": "JavaScript", "skill_category": "SKILL", "description": "Web development"},
            {"skill_name": "Docker", "skill_category": "SKILL", "description": "Containerization"},
            {"skill_name": "Data Science", "skill_category": "FIELD", "description": "Analytics field"},
            {"skill_name": "Machine Learning", "skill_category": "SKILL", "description": "AI/ML"},
            {"skill_name": "React", "skill_category": "SKILL", "description": "Frontend framework"}
        ]
    else:
        # ä½¿ç”¨å®Œå…¨éš¨æ©Ÿçš„æŠ€èƒ½(æ¨¡æ“¬çœŸå¯¦å ´æ™¯)
        import random
        import string
        suffix = ''.join(random.choices(string.ascii_lowercase, k=4))  # noqa: S311
        test_skills = [
            {"skill_name": f"UniqueSkill_{suffix}_1", "skill_category": "SKILL", "description": "Unique test 1"},
            {"skill_name": f"UniqueSkill_{suffix}_2", "skill_category": "SKILL", "description": "Unique test 2"},
            {"skill_name": f"UniqueSkill_{suffix}_3", "skill_category": "SKILL", "description": "Unique test 3"},
            {"skill_name": f"UniqueField_{suffix}_1", "skill_category": "FIELD", "description": "Unique test 4"},
            {"skill_name": f"UniqueField_{suffix}_2", "skill_category": "FIELD", "description": "Unique test 5"},
            {"skill_name": f"UniqueField_{suffix}_3", "skill_category": "FIELD", "description": "Unique test 6"}
        ]

    result["skills"] = [s["skill_name"] for s in test_skills]

    try:
        # Stage 1: ç²å– Singleton å¯¦ä¾‹
        tracker.start("get_singleton")
        await get_course_search_service()
        tracker.end("get_singleton")

        # Stage 2: å‰µå»º CourseAvailabilityChecker
        tracker.start("create_checker")
        checker = CourseAvailabilityChecker()
        tracker.end("create_checker")

        # Stage 3: åˆå§‹åŒ– Checker (åŒ…å« embedding client åˆå§‹åŒ–)
        tracker.start("initialize_checker")
        await checker.initialize()
        tracker.end("initialize_checker")

        # è¨˜éŒ„å¿«å–æª¢æŸ¥çš„æ™‚é–“
        tracker.start("cache_check")
        cached_results = checker._check_cache(test_skills)
        cache_hit_count = len(cached_results)
        tracker.end("cache_check")

        # éœ€è¦æŸ¥è©¢çš„æŠ€èƒ½
        uncached_skills = [s for s in test_skills if s["skill_name"] not in cached_results]

        if uncached_skills:
            # Stage 4: ç”Ÿæˆ embedding æ–‡æœ¬
            tracker.start("generate_embedding_text")
            query_texts = [
                checker._generate_embedding_text(skill)
                for skill in uncached_skills
            ]
            tracker.end("generate_embedding_text")

            # Stage 5: èª¿ç”¨ Embedding API
            tracker.start("embedding_api_call")
            embeddings = await checker._embedding_client.create_embeddings(query_texts)
            tracker.end("embedding_api_call")

            # Stage 6: ç²å–é€£æ¥æ± (å¦‚æœéœ€è¦)
            if not checker._connection_pool:
                tracker.start("get_connection_pool")
                from src.services.course_search_singleton import get_course_search_service
                course_service = await get_course_search_service()
                checker._connection_pool = course_service._connection_pool
                tracker.end("get_connection_pool")

            # Stage 7: ä¸¦è¡ŒæŸ¥è©¢è³‡æ–™åº«
            tracker.start("database_queries")
            tasks = []
            for emb, skill in zip(embeddings, uncached_skills, strict=False):
                tasks.append(checker._check_single_skill(
                    emb,
                    skill['skill_name'],
                    skill.get('skill_category', 'DEFAULT')
                ))

            # ç­‰å¾…æ‰€æœ‰æŸ¥è©¢å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            tracker.end("database_queries")

            # Stage 8: è™•ç†çµæœ
            tracker.start("process_results")
            for skill, query_result in zip(uncached_skills, results, strict=False):
                if isinstance(query_result, Exception):
                    skill["has_available_courses"] = False
                    skill["course_count"] = 0
                else:
                    skill["has_available_courses"] = query_result["has_courses"]
                    skill["course_count"] = query_result["count"]
            tracker.end("process_results")

        # è¨˜éŒ„çµæœ
        result["cache_hit_count"] = cache_hit_count
        result["cache_hit_rate"] = cache_hit_count / len(test_skills) if test_skills else 0
        result["uncached_count"] = len(uncached_skills)
        result["time_breakdown"] = tracker.get_breakdown()
        result["success"] = True

    except Exception as e:
        logger.error(f"æŸ¥è©¢å¤±æ•—: {e}")
        result["error"] = str(e)
        result["success"] = False
        result["time_breakdown"] = tracker.get_breakdown()

    return result


async def run_time_breakdown_analysis():
    """åŸ·è¡Œæ™‚é–“åˆ†è§£åˆ†æ"""
    logger.info("="*80)
    logger.info("ğŸ”¬ Course Availability æ™‚é–“åˆ†è§£åˆ†æ")
    logger.info("="*80)

    # é ç†±:åˆå§‹åŒ– Singleton
    logger.info("\nğŸ“ é ç†±éšæ®µ:åˆå§‹åŒ– Singleton...")
    from src.services.course_search_singleton import get_course_search_service
    await get_course_search_service()
    logger.info("âœ… Singleton å·²åˆå§‹åŒ–")

    # æ”¶é›†çµæœ
    all_results = []

    # æ¸¬è©¦ 5 æ¬¡éš¨æ©ŸæŠ€èƒ½(çœŸå¯¦å ´æ™¯)
    logger.info("\nğŸ“Š æ¸¬è©¦éš¨æ©ŸæŠ€èƒ½(æ¨¡æ“¬çœŸå¯¦å ´æ™¯)...")
    for i in range(5):
        logger.info(f"\nğŸ”„ åŸ·è¡Œç¬¬ {i+1}/5 æ¬¡æŸ¥è©¢(éš¨æ©ŸæŠ€èƒ½)...")
        result = await analyze_single_query(i+1, use_cached_skills=False)
        all_results.append(result)

        # æ‰“å°ç°¡è¦çµæœ
        if result["success"]:
            breakdown = result["time_breakdown"]
            logger.info(f"âœ… æŸ¥è©¢å®Œæˆ,ç¸½æ™‚é–“: {breakdown['total_ms']:.1f}ms")
            for stage in breakdown["stages"]:
                logger.info(f"   - {stage['stage']}: {stage['duration_ms']:.1f}ms")
        else:
            logger.info(f"âŒ æŸ¥è©¢å¤±æ•—: {result.get('error', 'Unknown error')}")

        # çŸ­æš«å»¶é²
        await asyncio.sleep(0.5)

    # æ¸¬è©¦ 5 æ¬¡å¿«å–æŠ€èƒ½(å°æ¯”)
    logger.info("\nğŸ“Š æ¸¬è©¦ç†±é–€æŠ€èƒ½(å°æ¯”å¿«å–æ•ˆæœ)...")
    for i in range(5):
        logger.info(f"\nğŸ”„ åŸ·è¡Œç¬¬ {i+1}/5 æ¬¡æŸ¥è©¢(ç†±é–€æŠ€èƒ½)...")
        result = await analyze_single_query(i+6, use_cached_skills=True)
        all_results.append(result)

        # æ‰“å°ç°¡è¦çµæœ
        if result["success"]:
            breakdown = result["time_breakdown"]
            logger.info(f"âœ… æŸ¥è©¢å®Œæˆ,ç¸½æ™‚é–“: {breakdown['total_ms']:.1f}ms")
            logger.info(f"   å¿«å–å‘½ä¸­ç‡: {result['cache_hit_rate']:.1%}")
            for stage in breakdown["stages"]:
                logger.info(f"   - {stage['stage']}: {stage['duration_ms']:.1f}ms")
        else:
            logger.info(f"âŒ æŸ¥è©¢å¤±æ•—: {result.get('error', 'Unknown error')}")

        # çŸ­æš«å»¶é²
        await asyncio.sleep(0.5)

    # ä¿å­˜è©³ç´°çµæœåˆ° JSON
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"/Users/yuwenhao/Documents/GitHub/azure_container/test/logs/time_breakdown_{timestamp}.json"

    # è¨ˆç®—çµ±è¨ˆè³‡æ–™
    random_times = [r["time_breakdown"]["total_ms"] for r in all_results[:5] if r["success"]]
    cached_times = [r["time_breakdown"]["total_ms"] for r in all_results[5:] if r["success"]]

    report = {
        "test_name": "Course Availability Time Breakdown Analysis",
        "timestamp": datetime.now(UTC).isoformat(),
        "iterations": all_results,
        "summary": {
            "random_skills": {
                "count": len(random_times),
                "avg_ms": round(sum(random_times) / len(random_times), 2) if random_times else 0,
                "min_ms": round(min(random_times), 2) if random_times else 0,
                "max_ms": round(max(random_times), 2) if random_times else 0
            },
            "cached_skills": {
                "count": len(cached_times),
                "avg_ms": round(sum(cached_times) / len(cached_times), 2) if cached_times else 0,
                "min_ms": round(min(cached_times), 2) if cached_times else 0,
                "max_ms": round(max(cached_times), 2) if cached_times else 0
            }
        },
        "stage_analysis": {}
    }

    # åˆ†æå„éšæ®µå¹³å‡è€—æ™‚
    stage_times = {}
    for result in all_results:
        if result["success"] and "time_breakdown" in result:
            for stage in result["time_breakdown"]["stages"]:
                stage_name = stage["stage"]
                if stage_name not in stage_times:
                    stage_times[stage_name] = []
                stage_times[stage_name].append(stage["duration_ms"])

    for stage_name, times in stage_times.items():
        report["stage_analysis"][stage_name] = {
            "avg_ms": round(sum(times) / len(times), 2),
            "min_ms": round(min(times), 2),
            "max_ms": round(max(times), 2),
            "occurrences": len(times)
        }

    # ä¿å­˜å ±å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š åˆ†æå®Œæˆ!")
    logger.info(f"ğŸ“ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    logger.info("="*80)

    # æ‰“å°ç¸½çµ
    logger.info("\nğŸ“ˆ ç¸½çµçµ±è¨ˆ:")
    logger.info(f"éš¨æ©ŸæŠ€èƒ½å¹³å‡è€—æ™‚: {report['summary']['random_skills']['avg_ms']:.1f}ms")
    logger.info(f"ç†±é–€æŠ€èƒ½å¹³å‡è€—æ™‚: {report['summary']['cached_skills']['avg_ms']:.1f}ms")
    logger.info("\nğŸ” éšæ®µåˆ†æ:")
    for stage, stats in report["stage_analysis"].items():
        logger.info(f"   {stage}: å¹³å‡ {stats['avg_ms']:.1f}ms (æœ€å° {stats['min_ms']:.1f}ms, æœ€å¤§ {stats['max_ms']:.1f}ms)")


if __name__ == "__main__":
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    from dotenv import load_dotenv
    load_dotenv()

    # åŸ·è¡Œåˆ†æ
    asyncio.run(run_time_breakdown_analysis())

#!/usr/bin/env python
"""
æ¸¬è©¦é€£æ¥æ± é ç†±æ•ˆæœ
"""
import asyncio
import logging
import sys
import time
from datetime import UTC, datetime
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def simulate_warmup():
    """æ¨¡æ“¬é ç†±éç¨‹"""
    from src.services.course_search_singleton import get_course_search_service

    logger.info("="*80)
    logger.info("ğŸ”¬ æ¸¬è©¦é€£æ¥æ± é ç†±æ•ˆæœ")
    logger.info("="*80)

    # Step 1: åˆå§‹åŒ–é€£æ¥æ± 
    logger.info("\nğŸ“ Step 1: åˆå§‹åŒ–é€£æ¥æ± ...")
    start_time = time.perf_counter()
    course_service = await get_course_search_service()
    pool = course_service._connection_pool
    init_time = (time.perf_counter() - start_time) * 1000
    logger.info(f"âœ… é€£æ¥æ± åˆå§‹åŒ–å®Œæˆ:{init_time:.1f}ms")

    # Step 2: åŸ·è¡Œé ç†±
    logger.info("\nğŸ“ Step 2: åŸ·è¡Œé ç†±ç¨‹åº...")
    warmup_start = time.perf_counter()

    if pool:
        # å¼·åˆ¶å‰µå»ºé€£æ¥
        connections = []
        num_connections = min(pool._maxsize, 5)
        logger.info(f"   å‰µå»º {num_connections} å€‹é€£æ¥...")

        for i in range(num_connections):
            conn = await pool.acquire()
            connections.append(conn)
            logger.info(f"   é€£æ¥ #{i+1} å·²å‰µå»º")

        # é ç†±æŸ¥è©¢
        logger.info("   åŸ·è¡Œé ç†±æŸ¥è©¢...")
        test_embedding = [0.1] * 1536

        for i, conn in enumerate(connections[:2]):
            try:
                query_start = time.perf_counter()
                count = await conn.fetchval(
                    """
                    SELECT COUNT(*)
                    FROM courses
                    WHERE platform = 'coursera'
                    AND embedding IS NOT NULL
                    AND 1 - (embedding <=> $1::vector) >= 0.01
                    LIMIT 1
                    """,
                    test_embedding
                )
                query_time = (time.perf_counter() - query_start) * 1000
                logger.info(f"   é ç†±æŸ¥è©¢ #{i+1} å®Œæˆ:{query_time:.1f}ms (æ‰¾åˆ° {count} ç­†)")
            except Exception as e:
                logger.error(f"   é ç†±æŸ¥è©¢ #{i+1} å¤±æ•—:{e}")

        # é‡‹æ”¾é€£æ¥
        for conn in connections:
            await pool.release(conn)

        warmup_time = (time.perf_counter() - warmup_start) * 1000
        logger.info(f"âœ… é ç†±å®Œæˆ:{warmup_time:.1f}ms")

    # Step 3: æ¸¬è©¦é ç†±å¾Œçš„æŸ¥è©¢é€Ÿåº¦
    logger.info("\nğŸ“ Step 3: æ¸¬è©¦é ç†±å¾Œçš„æŸ¥è©¢é€Ÿåº¦...")
    from src.services.course_availability import CourseAvailabilityChecker

    checker = CourseAvailabilityChecker()
    await checker.initialize()

    # åŸ·è¡Œ 5 æ¬¡æŸ¥è©¢æ¸¬è©¦
    query_times = []
    for i in range(5):
        test_skills = [
            {"skill_name": f"Python_{i}", "skill_category": "SKILL", "description": "Programming"},
            {"skill_name": f"JavaScript_{i}", "skill_category": "SKILL", "description": "Web"},
            {"skill_name": f"Docker_{i}", "skill_category": "SKILL", "description": "Container"},
            {"skill_name": f"DataScience_{i}", "skill_category": "FIELD", "description": "Analytics"},
            {"skill_name": f"MachineLearning_{i}", "skill_category": "FIELD", "description": "AI"},
            {"skill_name": f"CloudComputing_{i}", "skill_category": "FIELD", "description": "Cloud"}
        ]

        start = time.perf_counter()
        await checker.check_course_availability(test_skills)
        elapsed = (time.perf_counter() - start) * 1000
        query_times.append(elapsed)
        logger.info(f"   æŸ¥è©¢ #{i+1}: {elapsed:.1f}ms")

    # çµ±è¨ˆçµæœ
    logger.info("\nğŸ“Š æŸ¥è©¢é€Ÿåº¦çµ±è¨ˆ(é ç†±å¾Œ):")
    logger.info(f"   ç¬¬ä¸€æ¬¡æŸ¥è©¢: {query_times[0]:.1f}ms")
    logger.info(f"   å¹³å‡æŸ¥è©¢æ™‚é–“: {sum(query_times)/len(query_times):.1f}ms")
    logger.info(f"   æœ€å¿«æŸ¥è©¢: {min(query_times):.1f}ms")
    logger.info(f"   æœ€æ…¢æŸ¥è©¢: {max(query_times):.1f}ms")

    # æ¯”è¼ƒæœ‰ç„¡é ç†±çš„å·®ç•°
    logger.info("\nğŸ“ˆ é ç†±æ•ˆæœæ¯”è¼ƒ:")
    logger.info("   æœªé ç†±ç¬¬ä¸€æ¬¡æŸ¥è©¢: ~3000ms (æ ¹æ“šä¹‹å‰æ¸¬è©¦)")
    logger.info(f"   é ç†±å¾Œç¬¬ä¸€æ¬¡æŸ¥è©¢: {query_times[0]:.1f}ms")
    logger.info(f"   æ”¹å–„: {(1 - query_times[0]/3000) * 100:.1f}%")

    logger.info("\n" + "="*80)
    logger.info("ğŸ”¬ æ¸¬è©¦å®Œæˆ")
    logger.info("="*80)


if __name__ == "__main__":
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    from dotenv import load_dotenv
    load_dotenv()

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(simulate_warmup())

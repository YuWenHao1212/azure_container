#!/usr/bin/env python3
"""
Pre-commit validation script - Python version
ç›´æ¥åŸ·è¡Œæ¸¬è©¦è€Œä¸éœ€è¦èª¿ç”¨å¤šå€‹ shell è…³æœ¬
"""
# ruff: noqa: S603  # subprocess calls are safe in this test script
import json
import os
import subprocess
import sys
import tempfile
import time
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Colors for terminal output
class Colors:
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    RED = '\033[0;31m'
    YELLOW = '\033[0;33m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

@dataclass
class TestResult:
    """Store test result information"""
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    duration: float = 0.0
    failed_tests: list[str] = None

    def __post_init__(self):
        if self.failed_tests is None:
            self.failed_tests = []

    @property
    def total(self):
        return self.passed + self.failed + self.skipped

    @property
    def status(self):
        return "âœ…" if self.failed == 0 else "âŒ"

class PreCommitValidator:
    """Main pre-commit validation class"""

    def __init__(self):
        self.start_time = time.time()
        self.results = {}
        self.overall_failed = False

    def run_ruff_check(self) -> TestResult:
        """Run Ruff code quality check"""
        print(f"{Colors.BLUE}ğŸ“ Step 1/5: Running Ruff check...{Colors.RESET}")
        print("Checking src/ and test/ directories")

        start = time.time()
        result = TestResult()

        try:
            # Use absolute paths to make it clear these are controlled inputs
            src_path = str(project_root / "src")
            test_path = str(project_root / "test")
            cmd = ["ruff", "check", src_path, test_path, "--line-length=120"]
            process = subprocess.run(cmd, capture_output=True, text=True, shell=False, check=False)

            result.duration = time.time() - start

            if process.returncode == 0:
                result.passed = 1
                print(f"{Colors.GREEN}âœ… Ruff check passed{Colors.RESET}")
            else:
                result.failed = 1
                self.overall_failed = True
                print(f"{Colors.RED}âŒ Ruff check FAILED{Colors.RESET}")
                if process.stdout:
                    result.failed_tests = process.stdout.split('\n')[:5]  # First 5 errors
        except Exception as e:
            result.failed = 1
            self.overall_failed = True
            print(f"{Colors.RED}âŒ Ruff check error: {e}{Colors.RESET}")

        return result

    def run_pytest_tests(self, test_files: list[str], name: str) -> dict[str, TestResult]:
        """Run pytest on specified test files"""
        results = {"unit": TestResult(), "integration": TestResult()}

        # Set environment for testing
        env = os.environ.copy()
        env['PYTHONDONTWRITEBYTECODE'] = '1'
        env['TESTING'] = 'true'

        for test_file in test_files:
            test_path = project_root / test_file
            if not test_path.exists():
                continue

            # Determine if unit or integration test
            test_type = "unit" if "unit" in test_file else "integration"

            # Use tempfile for pytest report
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp_report:
                report_path = tmp_report.name

            # Run pytest with JSON output
            cmd = [
                sys.executable, "-m", "pytest",
                str(test_path),
                "-v", "--tb=short",
                "--json-report", f"--json-report-file={report_path}",
                "--no-header", "--no-summary", "-q"
            ]

            start = time.time()
            process = subprocess.run(cmd, capture_output=True, text=True, env=env, shell=False, check=False)
            duration = time.time() - start

            # Parse pytest JSON report if available
            try:
                with open(report_path) as f:
                    report = json.load(f)
                    summary = report.get("summary", {})
                    results[test_type].passed += summary.get("passed", 0)
                    results[test_type].failed += summary.get("failed", 0)
                    results[test_type].skipped += summary.get("skipped", 0)
                    results[test_type].duration += duration

                    # Collect failed test names
                    if summary.get("failed", 0) > 0:
                        for test in report.get("tests", []):
                            if test.get("outcome") == "failed":
                                results[test_type].failed_tests.append(test.get("nodeid", "Unknown"))
            except Exception:
                # Fallback: parse stdout if JSON report fails
                output = process.stdout
                if "passed" in output:
                    # Simple parsing from pytest output
                    import re
                    match = re.search(r'(\d+) passed', output)
                    if match:
                        results[test_type].passed += int(match.group(1))
                    match = re.search(r'(\d+) failed', output)
                    if match:
                        results[test_type].failed += int(match.group(1))
                        self.overall_failed = True

                results[test_type].duration += duration
            finally:
                # Clean up temp file
                if 'report_path' in locals() and os.path.exists(report_path):
                    os.unlink(report_path)

        return results

    def run_service_modules_tests(self) -> dict[str, TestResult]:
        """Run service module tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 2/5: Running Service Modules tests...{Colors.RESET}")

        service_tests = {
            "èªè¨€æª¢æ¸¬": ["test/unit/test_language_detection.py"],
            "Promptç®¡ç†": ["test/unit/test_prompt_manager.py", "test/unit/test_unified_prompt_service.py"],
            "é—œéµå­—æœå‹™": ["test/unit/test_keyword_extraction_service_v2.py"],
            "LLM Factory": ["test/unit/test_llm_factory.py"]
        }

        results = {}
        all_passed = True

        for service_name, test_files in service_tests.items():
            result = TestResult()
            for test_file in test_files:
                test_path = project_root / test_file
                if test_path.exists():
                    test_results = self.run_pytest_tests([test_file], service_name)
                    for test_type in test_results.values():
                        result.passed += test_type.passed
                        result.failed += test_type.failed
                        result.duration += test_type.duration
                        result.failed_tests.extend(test_type.failed_tests)

            results[service_name] = result
            if result.failed > 0:
                all_passed = False
                self.overall_failed = True

        if all_passed:
            print(f"{Colors.GREEN}âœ… Service Modules tests passed{Colors.RESET}")
        else:
            print(f"{Colors.RED}âŒ Service Modules tests FAILED{Colors.RESET}")

        return results

    def run_health_keyword_tests(self) -> dict[str, TestResult]:
        """Run Health & Keyword tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 3/5: Running Health & Keyword tests...{Colors.RESET}")

        test_files = [
            "test/unit/test_health.py",
            "test/unit/test_keyword_extraction.py",
            "test/integration/test_health_integration.py",
            "test/integration/test_keyword_extraction_language.py"
        ]

        results = self.run_pytest_tests(test_files, "health_keyword")

        if any(r.failed > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Health & Keyword tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Health & Keyword tests passed{Colors.RESET}")

        return results

    def run_index_calculation_tests(self) -> dict[str, TestResult]:
        """Run Index Calculation tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 4/5: Running Index Calculation tests...{Colors.RESET}")

        test_files = [
            "test/unit/test_index_calculation_v2.py",
            "test/integration/test_index_calculation_v2_api.py"
        ]

        results = self.run_pytest_tests(test_files, "index_calc")

        if any(r.failed > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Index Calculation tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Index Calculation tests passed{Colors.RESET}")

        return results

    def run_gap_analysis_tests(self) -> dict[str, TestResult]:
        """Run Gap Analysis tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 5/5: Running Gap Analysis tests...{Colors.RESET}")

        test_files = [
            "test/unit/test_gap_analysis_v2.py",
            "test/integration/test_gap_analysis_v2_integration_complete.py",
            "test/integration/test_error_handling_v2.py"
        ]

        results = self.run_pytest_tests(test_files, "gap_analysis")

        if any(r.failed > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Gap Analysis tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Gap Analysis tests passed{Colors.RESET}")

        return results

    def print_summary_table(self):
        """Print the final summary table"""
        total_duration = time.time() - self.start_time
        total_passed = 0
        total_failed = 0

        print("\n" + "=" * 70)
        print("â•”" + "â•" * 68 + "â•—")
        print("â•‘" + " " * 20 + "Pre-commit å®Œæ•´æ¸¬è©¦å ±å‘Š" + " " * 25 + "â•‘")
        print("â•š" + "â•" * 68 + "â•")
        print("\nğŸ“Š æ¸¬è©¦çµ±è¨ˆç¸½è¦½")
        print("=" * 70)
        print(f"| {'æ¸¬è©¦åˆ†é¡':<26} | {'é€šé':>5} | {'å¤±æ•—':>5} | {'ç¸½è¨ˆ':>5} | {'è€—æ™‚':>6} | {'ç‹€æ…‹':>4} |")
        print("|" + "-" * 28 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 8 + "|" + "-" * 6 + "|")

        # Ruff check
        ruff_result = self.results.get("ruff", TestResult())
        ruff_status = ruff_result.status
        print(f"| {'ğŸ” Ruff æª¢æŸ¥':<26} | {ruff_status:>5} | {'-':>5} | {'-':>5} | {ruff_result.duration:.1f}s{' ':>2} | {ruff_status:>4} |")
        print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # Service modules
        print(f"| {'ğŸ—ï¸ æœå‹™æ¨¡çµ„æ¸¬è©¦':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")
        service_results = self.results.get("service_modules", {})
        for service_name, result in service_results.items():
            total_passed += result.passed
            total_failed += result.failed
            print(f"|   â”œâ”€ {service_name:<22} | {result.passed:>5} | {result.failed:>5} | {result.total:>5} | {result.duration:.1f}s{' ':>2} | {result.status:>4} |")

        print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # API tests
        api_tests = [
            ("ğŸ©º Health & Keyword", "health_keyword"),
            ("ğŸ§® Index Calculation", "index_calc"),
            ("ğŸ“ˆ Gap Analysis", "gap_analysis")
        ]

        for display_name, key in api_tests:
            print(f"| {display_name:<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")
            results = self.results.get(key, {})

            for test_type in ["unit", "integration"]:
                if test_type in results:
                    result = results[test_type]
                    total_passed += result.passed
                    total_failed += result.failed
                    type_label = "å–®å…ƒæ¸¬è©¦ (UT)" if test_type == "unit" else "æ•´åˆæ¸¬è©¦ (IT)"
                    print(f"|   â”œâ”€ {type_label:<22} | {result.passed:>5} | {result.failed:>5} | {result.total:>5} | {result.duration:.1f}s{' ':>2} | {result.status:>4} |")

            print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # Total row
        total_tests = total_passed + total_failed
        overall_status = "âœ…" if total_failed == 0 else "âŒ"
        print("|" + "-" * 28 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 8 + "|" + "-" * 6 + "|")
        print(f"| {'ğŸ¯ ç¸½è¨ˆ':<26} | {total_passed:>5} | {total_failed:>5} | {total_tests:>5} | {total_duration:.1f}s{' ':>2} | {overall_status:>4} |")
        print("=" * 70)

        # Final message
        if total_failed == 0:
            print(f"\n{Colors.GREEN}ğŸ† æ‰€æœ‰æª¢æŸ¥é€šéï¼ä»£ç¢¼å“è³ªå„ªè‰¯ï¼Œæº–å‚™æäº¤ã€‚{Colors.RESET}")
        else:
            print(f"\n{Colors.RED}âŒ ç™¼ç¾ {total_failed} å€‹æ¸¬è©¦å¤±æ•—ï¼Œç„¡æ³•æäº¤{Colors.RESET}")
            self.print_failure_details()

    def print_failure_details(self):
        """Print detailed failure information"""
        print(f"\n{Colors.YELLOW}ğŸ” å¤±æ•—æ¸¬è©¦è©³ç´°è³‡è¨Š{Colors.RESET}")
        print("=" * 70)

        # Service module failures
        service_results = self.results.get("service_modules", {})
        service_failures = [name for name, r in service_results.items() if r.failed > 0]
        if service_failures:
            print(f"ğŸ—ï¸ æœå‹™æ¨¡çµ„å¤±æ•—: {', '.join(service_failures)}")

        # API test failures
        for display_name, key in [("ğŸ©º Health & Keyword", "health_keyword"),
                                  ("ğŸ§® Index Calculation", "index_calc"),
                                  ("ğŸ“ˆ Gap Analysis", "gap_analysis")]:
            results = self.results.get(key, {})
            has_failures = False

            for test_type in ["unit", "integration"]:
                if test_type in results and results[test_type].failed > 0:
                    if not has_failures:
                        print(f"\n{display_name} å¤±æ•—æ¸¬è©¦:")
                        has_failures = True

                    type_label = "å–®å…ƒæ¸¬è©¦" if test_type == "unit" else "æ•´åˆæ¸¬è©¦"
                    failed_tests = results[test_type].failed_tests[:3]  # Show first 3
                    if failed_tests:
                        print(f"   â”œâ”€ {type_label}: {', '.join(failed_tests)}")

        print(f"\n{Colors.YELLOW}ğŸ“‹ å»ºè­°ä¿®å¾©æ­¥é©Ÿ:{Colors.RESET}")
        print("1. æŸ¥çœ‹è©³ç´°æ¸¬è©¦è¼¸å‡º")
        print("2. ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦")
        print("3. é‡æ–°åŸ·è¡Œæ¸¬è©¦ç¢ºèªä¿®å¾©")
        print("4. å†æ¬¡å˜—è©¦æäº¤")

    def run(self):
        """Main execution method"""
        print(f"\n{Colors.BOLD}ğŸš¨ Pre-commit validation starting...{Colors.RESET}")
        print("=" * 70)
        print()

        # Step 1: Ruff check
        self.results["ruff"] = self.run_ruff_check()

        # Step 2: Service Modules
        self.results["service_modules"] = self.run_service_modules_tests()

        # Step 3: Health & Keyword
        self.results["health_keyword"] = self.run_health_keyword_tests()

        # Step 4: Index Calculation
        self.results["index_calc"] = self.run_index_calculation_tests()

        # Step 5: Gap Analysis
        self.results["gap_analysis"] = self.run_gap_analysis_tests()

        # Print summary
        self.print_summary_table()

        # Exit with appropriate code
        if self.overall_failed:
            print(f"\n{Colors.RED}â›” BLOCKING COMMIT - Fix test failures first{Colors.RESET}")
            print(f"\n{Colors.YELLOW}ğŸ”§ Next steps:{Colors.RESET}")
            print("1. Fix the failed tests shown in the summary above")
            print("2. Run individual test suites to verify fixes")
            print("3. Re-run this pre-commit check")
            print("4. Once all tests pass, try committing again")
            sys.exit(1)
        else:
            sys.exit(0)

if __name__ == "__main__":
    validator = PreCommitValidator()
    validator.run()

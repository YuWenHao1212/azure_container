#!/usr/bin/env python3
"""
Advanced Pre-commit validation script
ä½¿ç”¨ pytest ç¨‹å¼åŒ– API ç›´æ¥åŸ·è¡Œæ¸¬è©¦ä¸¦æ”¶é›†è©³ç´°çµæœ
"""
# ruff: noqa: S603  # subprocess calls are safe in this test script
import asyncio
import os
import subprocess
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

import pytest
from _pytest.config import ExitCode
from _pytest.terminal import TerminalReporter

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

# Colors for terminal output
class Colors:
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    RED = '\033[0;31m'
    YELLOW = '\033[0;33m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

@dataclass
class TestResult:
    """Store test result information"""
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    errors: int = 0
    duration: float = 0.0
    failed_tests: list[str] = field(default_factory=list)

    @property
    def total(self):
        return self.passed + self.failed + self.skipped

    @property
    def status(self):
        return "âœ…" if self.failed == 0 and self.errors == 0 else "âŒ"

class CustomPlugin:
    """Custom pytest plugin to collect test results"""

    def __init__(self):
        self.results = TestResult()
        self.start_time = None

    def pytest_sessionstart(self, session):
        self.start_time = time.time()

    def pytest_sessionfinish(self, session):
        self.results.duration = time.time() - self.start_time

    def pytest_runtest_logreport(self, report):
        if report.when == "call":
            if report.passed:
                self.results.passed += 1
            elif report.failed:
                self.results.failed += 1
                self.results.failed_tests.append(report.nodeid)
            elif report.skipped:
                self.results.skipped += 1

class AdvancedPreCommitValidator:
    """Advanced pre-commit validation with pytest API"""

    def __init__(self):
        self.start_time = time.time()
        self.results = {}
        self.overall_failed = False

        # Set testing environment
        os.environ['TESTING'] = 'true'
        os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
        os.environ['USE_V2_IMPLEMENTATION'] = 'true'

    def run_ruff_check(self) -> TestResult:
        """Run Ruff code quality check"""
        print(f"{Colors.BLUE}ğŸ“ Step 1/5: Running Ruff check...{Colors.RESET}")
        print("Checking src/ and test/ directories")

        start = time.time()
        result = TestResult()

        try:
            # Use absolute paths to make it clear these are controlled inputs
            src_path = str(project_root / "src")
            test_path = str(project_root / "test")
            cmd = ["ruff", "check", src_path, test_path, "--line-length=120", "--output-format=concise"]
            process = subprocess.run(cmd, capture_output=True, text=True, shell=False, check=False)

            result.duration = time.time() - start

            if process.returncode == 0:
                result.passed = 1
                print(f"{Colors.GREEN}âœ… Ruff check passed{Colors.RESET}")
            else:
                # Parse output to distinguish between errors and warnings
                lines = process.stdout.strip().split('\n') if process.stdout else []
                errors = []
                warnings = []

                for line in lines:
                    if line and not line.startswith('Found'):
                        if 'S603' in line or 'S108' in line:  # Security warnings
                            warnings.append(line)
                        else:
                            errors.append(line)

                if errors:
                    result.failed = 1
                    self.overall_failed = True
                    print(f"{Colors.RED}âŒ Ruff check FAILED{Colors.RESET}")
                    print(f"  Errors found: {len(errors)}")
                    for error in errors[:3]:  # Show first 3 errors
                        print(f"    {error}")
                elif warnings:
                    result.passed = 1
                    print(f"{Colors.YELLOW}âš ï¸  Ruff check passed with warnings{Colors.RESET}")
                    print(f"  Security warnings: {len(warnings)}")
                    # Show warning summary
                    warning_types = {}
                    for warning in warnings:
                        if 'S603' in warning:
                            warning_types['S603'] = warning_types.get('S603', 0) + 1
                        elif 'S108' in warning:
                            warning_types['S108'] = warning_types.get('S108', 0) + 1
                    for wtype, count in warning_types.items():
                        if wtype == 'S603':
                            print(f"    {count} x S603: subprocess-without-shell-equals-true")
                        elif wtype == 'S108':
                            print(f"    {count} x S108: hardcoded-temp-file")

                result.failed_tests = lines[:5]  # Store for summary

        except FileNotFoundError:
            print(f"{Colors.YELLOW}âš ï¸  Ruff not found, skipping code check{Colors.RESET}")
            result.skipped = 1
        except Exception as e:
            result.errors = 1
            self.overall_failed = True
            print(f"{Colors.RED}âŒ Ruff check error: {e}{Colors.RESET}")

        return result

    def run_pytest_programmatically(self, test_paths: list[str], test_name: str) -> TestResult:
        """Run pytest programmatically and collect results"""
        plugin = CustomPlugin()

        # Configure pytest arguments for minimal output
        args = [
            "-q",  # Quiet mode - minimal output
            "--tb=no",  # No traceback
            "--no-header", "--no-summary",
            "--disable-warnings",
            "--color=no"  # Disable color to avoid parsing issues
        ]

        # Suppress output for integration tests to keep output clean
        if "integration" in test_name or "IT" in test_name:
            args.append("--capture=sys")  # Capture stdout/stderr
        else:
            args.append("-s")  # No capture for unit tests

        # Add test paths
        for path in test_paths:
            test_file = project_root / path
            if test_file.exists():
                args.append(str(test_file))

        # Check if we have any test files (args has base options + capture option + test files)
        base_args_count = 7 if "-s" in args or "--capture=sys" in args else 6
        if len(args) == base_args_count:  # No test files found
            return TestResult(skipped=1)

        # Redirect pytest output to suppress it
        import contextlib
        import io

        # Capture pytest output to suppress the "X passed, Y warnings in Z.XXs" messages
        output_buffer = io.StringIO()

        # Run pytest with custom plugin and captured output
        try:
            with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):
                exit_code = pytest.main(args, plugins=[plugin])

            # Handle exit codes
            if exit_code == ExitCode.TESTS_FAILED:
                self.overall_failed = True
            elif exit_code == ExitCode.INTERNAL_ERROR:
                plugin.results.errors += 1
                self.overall_failed = True

        except Exception as e:
            plugin.results.errors += 1
            self.overall_failed = True
            print(f"Error running {test_name}: {e}")

        return plugin.results

    def run_service_modules_tests(self) -> dict[str, TestResult]:
        """Run service module tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 2/5: Running Service Modules tests...{Colors.RESET}")

        # Based on the actual test files in test/unit/services/
        service_tests = {
            "èªè¨€æª¢æ¸¬": [
                "test/unit/services/test_language_detection_service.py"
            ],
            "Promptç®¡ç†": [
                "test/unit/services/test_unified_prompt_service.py"
            ],
            "é—œéµå­—æœå‹™": [
                "test/unit/services/test_keyword_service_integration.py"
            ],
            "LLM Factory": [
                "test/unit/services/test_llm_factory_service.py"
            ]
        }

        results = {}
        all_passed = True

        for service_name, test_files in service_tests.items():
            # Check if files exist and run pytest
            existing_files = [f for f in test_files if (project_root / f).exists()]

            if existing_files:
                # Show compact collection message
                print(f"  {service_name}: ", end="", flush=True)

                # Run pytest and get actual results
                result = self.run_pytest_programmatically(existing_files, service_name)

                # Show inline result
                if result.total > 0:
                    status_icon = "âœ…" if result.failed == 0 else "âŒ"
                    print(f"collected {result.total} items, {result.passed} passed {status_icon}")
            else:
                # If files don't exist, create empty result
                print(f"Warning: No test files found for {service_name}")
                result = TestResult()
                result.skipped = 1
                result.duration = 0.0

            results[service_name] = result
            if result.failed > 0 or result.errors > 0:
                all_passed = False
                self.overall_failed = True

        if all_passed:
            print(f"{Colors.GREEN}âœ… Service Modules tests passed{Colors.RESET}")
        else:
            print(f"{Colors.RED}âŒ Service Modules tests FAILED{Colors.RESET}")

        return results

    def run_health_keyword_tests(self) -> dict[str, TestResult]:
        """Run Health & Keyword tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 3/5: Running Health & Keyword tests...{Colors.RESET}")

        unit_files = [
            "test/unit/test_health.py",
            "test/unit/test_keyword_extraction.py"
        ]

        integration_files = [
            "test/integration/test_health_integration.py",
            "test/integration/test_keyword_extraction_language.py"
        ]

        # Run unit tests
        print("  Unit Tests: ", end="", flush=True)
        unit_result = self.run_pytest_programmatically(unit_files, "health_keyword_unit")
        if unit_result.total > 0:
            status = "âœ…" if unit_result.failed == 0 else "âŒ"
            print(f"collected {unit_result.total} items, {unit_result.passed} passed {status}")

        # Run integration tests
        print("  Integration Tests: ", end="", flush=True)
        integration_result = self.run_pytest_programmatically(integration_files, "health_keyword_integration")
        if integration_result.total > 0:
            status = "âœ…" if integration_result.failed == 0 else "âŒ"
            print(f"collected {integration_result.total} items, {integration_result.passed} passed {status}")

        results = {
            "unit": unit_result,
            "integration": integration_result
        }

        if any(r.failed > 0 or r.errors > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Health & Keyword tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Health & Keyword tests passed{Colors.RESET}")

        return results

    def run_index_calculation_tests(self) -> dict[str, TestResult]:
        """Run Index Calculation tests"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 4/5: Running Index Calculation tests...{Colors.RESET}")

        unit_files = ["test/unit/test_index_calculation_v2.py"]
        integration_files = ["test/integration/test_index_calculation_v2_api.py"]

        # Run unit tests
        print("  Unit Tests: ", end="", flush=True)
        unit_result = self.run_pytest_programmatically(unit_files, "index_calc_unit")
        if unit_result.total > 0:
            status = "âœ…" if unit_result.failed == 0 else "âŒ"
            print(f"collected {unit_result.total} items, {unit_result.passed} passed {status}")

        # Run integration tests
        print("  Integration Tests: ", end="", flush=True)
        integration_result = self.run_pytest_programmatically(integration_files, "index_calc_integration")
        if integration_result.total > 0:
            status = "âœ…" if integration_result.failed == 0 else "âŒ"
            print(f"collected {integration_result.total} items, {integration_result.passed} passed {status}")

        results = {
            "unit": unit_result,
            "integration": integration_result
        }

        if any(r.failed > 0 or r.errors > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Index Calculation tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Index Calculation tests passed{Colors.RESET}")

        return results

    def run_gap_analysis_tests(self) -> dict[str, TestResult]:
        """Run Gap Analysis tests with timeout handling"""
        print(f"\n{Colors.BLUE}ğŸ“ Step 5/5: Running Gap Analysis tests...{Colors.RESET}")

        unit_files = ["test/unit/test_gap_analysis_v2.py"]
        integration_files = [
            "test/integration/test_gap_analysis_v2_integration_complete.py",
            "test/integration/test_error_handling_v2.py"
        ]

        # Run unit tests
        print("  Unit Tests: ", end="", flush=True)
        unit_result = self.run_pytest_programmatically(unit_files, "gap_analysis_unit")
        if unit_result.total > 0:
            status = "âœ…" if unit_result.failed == 0 else "âŒ"
            print(f"collected {unit_result.total} items, {unit_result.passed} passed {status}")

        # Run integration tests with timeout handling
        print("  Integration Tests: ", end="", flush=True)
        integration_result = TestResult()

        # Note: Integration tests might hang, so we set a timeout
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError("Integration tests timed out")

        # Set timeout for 30 seconds
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(30)

        try:
            integration_result = self.run_pytest_programmatically(integration_files, "gap_analysis_integration")
            signal.alarm(0)  # Cancel alarm
            if integration_result.total > 0:
                status = "âœ…" if integration_result.failed == 0 else "âŒ"
                print(f"collected {integration_result.total} items, {integration_result.passed} passed {status}")
        except TimeoutError:
            print(f"{Colors.YELLOW}timed out (27 tests skipped) âš ï¸{Colors.RESET}")
            integration_result.skipped = 27  # Expected 27 integration tests
            integration_result.duration = 30.0
            signal.alarm(0)  # Cancel alarm

        results = {
            "unit": unit_result,
            "integration": integration_result
        }

        if any(r.failed > 0 or r.errors > 0 for r in results.values()):
            print(f"{Colors.RED}âŒ Gap Analysis tests FAILED{Colors.RESET}")
            self.overall_failed = True
        else:
            print(f"{Colors.GREEN}âœ… Gap Analysis tests passed{Colors.RESET}")

        return results

    def print_summary_table(self):
        """Print the final summary table matching the shell script format"""
        total_duration = time.time() - self.start_time
        total_passed = 0
        total_failed = 0

        # Print header
        print("\n" + "=" * 71)
        print("â•”" + "â•" * 69 + "â•—")
        print("â•‘" + " " * 20 + "Pre-commit å®Œæ•´æ¸¬è©¦å ±å‘Š" + " " * 25 + "â•‘")
        print("â•š" + "â•" * 69 + "â•")
        print("\nğŸ“Š æ¸¬è©¦çµ±è¨ˆç¸½è¦½")
        print("=" * 71)

        # Table header
        print(f"| {'æ¸¬è©¦åˆ†é¡':<26} | {'é€šé':>5} | {'å¤±æ•—':>5} | {'ç¸½è¨ˆ':>5} | {'è€—æ™‚':>6} | {'ç‹€æ…‹':>4} |")
        print("|" + "-" * 28 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 8 + "|" + "-" * 6 + "|")

        # Ruff check
        ruff_result = self.results.get("ruff", TestResult())
        if ruff_result.skipped:
            print(f"| {'ğŸ” Ruff æª¢æŸ¥':<26} | {'â­ï¸':>5} | {'-':>5} | {'-':>5} | {ruff_result.duration:.1f}s{' ':>2} | {'â­ï¸':>4} |")
        else:
            print(f"| {'ğŸ” Ruff æª¢æŸ¥':<26} | {ruff_result.status:>5} | {'-':>5} | {'-':>5} | {ruff_result.duration:.1f}s{' ':>2} | {ruff_result.status:>4} |")

        print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # Service modules
        print(f"| {'ğŸ—ï¸ æœå‹™æ¨¡çµ„æ¸¬è©¦':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")
        service_results = self.results.get("service_modules", {})

        for service_name, result in service_results.items():
            total_passed += result.passed
            total_failed += result.failed
            print(f"|   â”œâ”€ {service_name:<22} | {result.passed:>5} | {result.failed:>5} | {result.total:>5} | {result.duration:.1f}s{' ':>2} | {result.status:>4} |")

        print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # API tests
        api_tests = [
            ("ğŸ©º Health & Keyword", "health_keyword"),
            ("ğŸ§® Index Calculation", "index_calc"),
            ("ğŸ“ˆ Gap Analysis", "gap_analysis")
        ]

        for display_name, key in api_tests:
            print(f"| {display_name:<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")
            results = self.results.get(key, {})

            for test_type in ["unit", "integration"]:
                if test_type in results:
                    result = results[test_type]
                    total_passed += result.passed
                    total_failed += result.failed
                    type_label = "å–®å…ƒæ¸¬è©¦ (UT)" if test_type == "unit" else "æ•´åˆæ¸¬è©¦ (IT)"
                    print(f"|   â”œâ”€ {type_label:<22} | {result.passed:>5} | {result.failed:>5} | {result.total:>5} | {result.duration:.1f}s{' ':>2} | {result.status:>4} |")

            print(f"| {'':<26} | {'':<5} | {'':<5} | {'':<5} | {'':<6} | {'':<4} |")

        # Total row
        total_tests = total_passed + total_failed
        overall_status = "âœ…" if total_failed == 0 and not self.overall_failed else "âŒ"

        print("|" + "-" * 28 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 7 + "|" + "-" * 8 + "|" + "-" * 6 + "|")
        print(f"| {'ğŸ¯ ç¸½è¨ˆ':<26} | {total_passed:>5} | {total_failed:>5} | {total_tests:>5} | {total_duration:.1f}s{' ':>2} | {overall_status:>4} |")
        print("=" * 71)

        # Final message
        if not self.overall_failed:
            print(f"\n{Colors.GREEN}ğŸ† æ‰€æœ‰æª¢æŸ¥é€šéï¼ä»£ç¢¼å“è³ªå„ªè‰¯ï¼Œæº–å‚™æäº¤ã€‚{Colors.RESET}")
        else:
            print(f"\n{Colors.RED}âŒ ç™¼ç¾ {total_failed} å€‹æ¸¬è©¦å¤±æ•—ï¼Œç„¡æ³•æäº¤{Colors.RESET}")
            self.print_failure_details()

    def print_failure_details(self):
        """Print detailed failure information"""
        has_failures = False

        # Check for any failures
        for key in ["service_modules", "health_keyword", "index_calc", "gap_analysis"]:
            if key in self.results:
                if isinstance(self.results[key], dict):
                    for result in self.results[key].values():
                        if result.failed > 0 or result.errors > 0:
                            has_failures = True
                            break
                elif self.results[key].failed > 0 or self.results[key].errors > 0:
                    has_failures = True
                    break

        if not has_failures:
            return

        print(f"\n{Colors.YELLOW}ğŸ” å¤±æ•—æ¸¬è©¦è©³ç´°è³‡è¨Š{Colors.RESET}")
        print("=" * 71)

        # Service module failures
        if "service_modules" in self.results:
            service_results = self.results["service_modules"]
            failed_services = []
            for name, result in service_results.items():
                if result.failed > 0:
                    failed_services.append(f"{name}({result.failed})")

            if failed_services:
                print(f"ğŸ—ï¸ æœå‹™æ¨¡çµ„å¤±æ•—: {', '.join(failed_services)}")

        # API test failures
        for display_name, key in [
            ("ğŸ©º Health & Keyword", "health_keyword"),
            ("ğŸ§® Index Calculation", "index_calc"),
            ("ğŸ“ˆ Gap Analysis", "gap_analysis")
        ]:
            if key in self.results:
                results = self.results[key]
                if isinstance(results, dict):
                    unit_failed = results.get("unit", TestResult()).failed
                    integration_failed = results.get("integration", TestResult()).failed

                    if unit_failed > 0 or integration_failed > 0:
                        print(f"\n{display_name} å¤±æ•—æ¸¬è©¦:")
                        if unit_failed > 0:
                            failed_tests = results["unit"].failed_tests[:3]
                            test_names = [t.split("::")[-1] for t in failed_tests]
                            print(f"   â”œâ”€ å–®å…ƒæ¸¬è©¦: {', '.join(test_names)}")
                        if integration_failed > 0:
                            failed_tests = results["integration"].failed_tests[:3]
                            test_names = [t.split("::")[-1] for t in failed_tests]
                            print(f"   â”œâ”€ æ•´åˆæ¸¬è©¦: {', '.join(test_names)}")

        print(f"\n{Colors.YELLOW}ğŸ“‹ å»ºè­°ä¿®å¾©æ­¥é©Ÿ:{Colors.RESET}")
        print("1. æŸ¥çœ‹è©³ç´°æ¸¬è©¦è¼¸å‡º")
        print("2. ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦")
        print("3. é‡æ–°åŸ·è¡Œæ¸¬è©¦ç¢ºèªä¿®å¾©")
        print("4. å†æ¬¡å˜—è©¦æäº¤")

    def run(self):
        """Main execution method"""
        print(f"\n{Colors.BOLD}ğŸš¨ Pre-commit validation starting...{Colors.RESET}")
        print("=" * 70)
        print()

        # Step 1: Ruff check
        self.results["ruff"] = self.run_ruff_check()

        # Step 2: Service Modules
        self.results["service_modules"] = self.run_service_modules_tests()

        # Step 3: Health & Keyword
        self.results["health_keyword"] = self.run_health_keyword_tests()

        # Step 4: Index Calculation
        self.results["index_calc"] = self.run_index_calculation_tests()

        # Step 5: Gap Analysis
        self.results["gap_analysis"] = self.run_gap_analysis_tests()

        # Print summary
        print("\n" + "=" * 70)
        self.print_summary_table()

        # Exit with appropriate code
        if self.overall_failed:
            print(f"\n{Colors.RED}â›” BLOCKING COMMIT - Fix test failures first{Colors.RESET}")
            print(f"\n{Colors.YELLOW}ğŸ”§ Next steps:{Colors.RESET}")
            print("1. Fix the failed tests shown in the summary above")
            print("2. Run individual test suites to verify fixes")
            print("3. Re-run this pre-commit check")
            print("4. Once all tests pass, try committing again")
            sys.exit(1)
        else:
            sys.exit(0)

if __name__ == "__main__":
    validator = AdvancedPreCommitValidator()
    validator.run()

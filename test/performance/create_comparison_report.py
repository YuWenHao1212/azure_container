#!/usr/bin/env python3
"""
Create comparison report between v1.0.0 and v1.0.1 performance tests
"""

import json
from datetime import datetime
from pathlib import Path

import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np


def load_test_results():
    """Load both test result files"""
    results_dir = Path('docs/issues/index-cal-gap-analysis-evolution/performance')

    # Load v1.0.0 results (earlier test)
    v100_file = results_dir / 'performance_results_20250816_133242.json'
    with open(v100_file) as f:
        v100_data = json.load(f)

    # Load v1.0.1 results (new test)
    v101_file = results_dir / 'performance_v101_20250816_143848.json'
    with open(v101_file) as f:
        v101_data = json.load(f)

    return v100_data, v101_data

def create_comparison_chart(v100_data, v101_data):
    """Create visual comparison between v1.0.0 and v1.0.1"""

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Performance Comparison: v1.0.0 vs v1.0.1 Resume Structure Analyzer', fontsize=16, fontweight='bold')

    # 1. Response Time Comparison
    ax1 = axes[0, 0]

    # Extract response times
    v100_times = v100_data['statistics']['all_times']
    v101_times = [r['total_time'] for r in v101_data['test_results']]

    companies = [r['company'] for r in v101_data['test_results'][:10]]
    v100_subset = v100_times[:10]
    v101_subset = v101_times[:10]

    x = np.arange(len(companies))
    width = 0.35

    bars1 = ax1.bar(x - width/2, v100_subset, width, label='v1.0.0', color='#FF6B6B', alpha=0.8)
    bars2 = ax1.bar(x + width/2, v101_subset, width, label='v1.0.1', color='#4ECDC4', alpha=0.8)

    ax1.set_xlabel('Test Case')
    ax1.set_ylabel('Response Time (seconds)')
    ax1.set_title('Response Times by Test Case (First 10)')
    ax1.set_xticks(x)
    ax1.set_xticklabels(companies, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Add value labels
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom', fontsize=8)
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}', ha='center', va='bottom', fontsize=8)

    # 2. Statistical Comparison
    ax2 = axes[0, 1]

    metrics = ['P50', 'P95', 'Mean', 'Min', 'Max']
    v100_values = [
        v100_data['statistics']['p50'],
        v100_data['statistics']['p95'],
        v100_data['statistics']['mean_time'],
        v100_data['statistics']['min_time'],
        v100_data['statistics']['max_time']
    ]
    v101_values = [
        v101_data['statistics']['p50'],
        v101_data['statistics']['p95'],
        v101_data['statistics']['mean_time'],
        v101_data['statistics']['min_time'],
        v101_data['statistics']['max_time']
    ]

    x = np.arange(len(metrics))
    width = 0.35

    bars1 = ax2.bar(x - width/2, v100_values, width, label='v1.0.0', color='#FF6B6B', alpha=0.8)
    bars2 = ax2.bar(x + width/2, v101_values, width, label='v1.0.1', color='#4ECDC4', alpha=0.8)

    ax2.set_xlabel('Metric')
    ax2.set_ylabel('Time (seconds)')
    ax2.set_title('Statistical Comparison')
    ax2.set_xticks(x)
    ax2.set_xticklabels(metrics)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Add value labels and improvement percentages
    for i, (bar1, bar2) in enumerate(zip(bars1, bars2, strict=False)):
        h1 = bar1.get_height()
        h2 = bar2.get_height()
        ax2.text(bar1.get_x() + bar1.get_width()/2., h1,
                f'{h1:.2f}', ha='center', va='bottom', fontsize=9)
        ax2.text(bar2.get_x() + bar2.get_width()/2., h2,
                f'{h2:.2f}', ha='center', va='bottom', fontsize=9)

        # Show improvement
        improvement = ((h1 - h2) / h1) * 100
        if improvement > 0:
            ax2.text(x[i], max(h1, h2) + 0.5,
                    f'â†“{improvement:.1f}%', ha='center', va='bottom',
                    color='green', fontweight='bold', fontsize=10)
        elif improvement < 0:
            ax2.text(x[i], max(h1, h2) + 0.5,
                    f'â†‘{abs(improvement):.1f}%', ha='center', va='bottom',
                    color='red', fontweight='bold', fontsize=10)

    # 3. Distribution Comparison
    ax3 = axes[1, 0]

    ax3.hist(v100_times, bins=10, alpha=0.5, label='v1.0.0', color='#FF6B6B', edgecolor='black')
    ax3.hist(v101_times, bins=10, alpha=0.5, label='v1.0.1', color='#4ECDC4', edgecolor='black')
    ax3.axvline(v100_data['statistics']['p50'], color='#FF6B6B', linestyle='--', linewidth=2, label='v1.0.0 P50')
    ax3.axvline(v101_data['statistics']['p50'], color='#4ECDC4', linestyle='--', linewidth=2, label='v1.0.1 P50')
    ax3.set_xlabel('Response Time (seconds)')
    ax3.set_ylabel('Frequency')
    ax3.set_title('Response Time Distribution')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Improvement Summary
    ax4 = axes[1, 1]
    ax4.axis('off')

    # Calculate improvements
    p50_improvement = ((v100_data['statistics']['p50'] - v101_data['statistics']['p50']) / v100_data['statistics']['p50']) * 100
    p95_improvement = ((v100_data['statistics']['p95'] - v101_data['statistics']['p95']) / v100_data['statistics']['p95']) * 100
    mean_improvement = ((v100_data['statistics']['mean_time'] - v101_data['statistics']['mean_time']) / v100_data['statistics']['mean_time']) * 100

    # Create summary text
    summary_text = f"""
    PERFORMANCE IMPROVEMENT SUMMARY
    =====================================

    Resume Structure Analyzer Prompt Version
    â€¢ v1.0.0 â†’ v1.0.1

    Response Time Improvements:
    â€¢ P50: {v100_data['statistics']['p50']:.2f}s â†’ {v101_data['statistics']['p50']:.2f}s ({p50_improvement:+.1f}%)
    â€¢ P95: {v100_data['statistics']['p95']:.2f}s â†’ {v101_data['statistics']['p95']:.2f}s ({p95_improvement:+.1f}%)
    â€¢ Mean: {v100_data['statistics']['mean_time']:.2f}s â†’ {v101_data['statistics']['mean_time']:.2f}s ({mean_improvement:+.1f}%)

    Key Achievements:
    â€¢ {"âœ… Significant improvement" if p50_improvement > 10 else "âš ï¸ Moderate improvement" if p50_improvement > 0 else "âŒ No improvement"} in P50 response time
    â€¢ {"âœ… Better consistency" if p95_improvement > 10 else "âš ï¸ Similar consistency" if p95_improvement > -5 else "âŒ Less consistent"} in P95 times
    â€¢ {"âœ… Overall faster" if mean_improvement > 10 else "âš ï¸ Slightly faster" if mean_improvement > 0 else "âŒ Slower"} average performance

    Test Configuration:
    â€¢ Tests: 20 diverse job descriptions
    â€¢ Environment: Azure Container Apps (Production)
    â€¢ Region: Japan East
    """

    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()

    # Save figure
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = Path('docs/issues/index-cal-gap-analysis-evolution/performance')
    output_file = output_dir / f'comparison_chart_{timestamp}.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"âœ… Comparison chart saved to: {output_file}")

    return output_file

def create_comparison_report(v100_data, v101_data):
    """Create detailed comparison report"""

    # Calculate improvements
    p50_improvement = ((v100_data['statistics']['p50'] - v101_data['statistics']['p50']) / v100_data['statistics']['p50']) * 100
    p95_improvement = ((v100_data['statistics']['p95'] - v101_data['statistics']['p95']) / v100_data['statistics']['p95']) * 100
    mean_improvement = ((v100_data['statistics']['mean_time'] - v101_data['statistics']['mean_time']) / v100_data['statistics']['mean_time']) * 100

    report = f"""# æ€§èƒ½æ¯”è¼ƒå ±å‘Š - Resume Structure Analyzer v1.0.1 å‡ç´š

## åŸ·è¡Œæ‘˜è¦

æœ¬å ±å‘Šæ¯”è¼ƒ Resume Structure Analyzer å¾ v1.0.0 å‡ç´šåˆ° v1.0.1 å¾Œçš„æ€§èƒ½è®ŠåŒ–ã€‚

**æ¸¬è©¦æ—¥æœŸ**: 2025-08-16
**æ¸¬è©¦ç’°å¢ƒ**: Azure Container Apps Production (Japan East)
**æ¸¬è©¦æ•¸é‡**: 20 å€‹ä¸åŒè·ç¼ºæè¿°

## ğŸ“Š é—œéµæ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒ

| æŒ‡æ¨™ | v1.0.0 | v1.0.1 | æ”¹å–„å¹…åº¦ | ç‹€æ…‹ |
|------|--------|--------|---------|------|
| **P50 (ä¸­ä½æ•¸)** | {v100_data['statistics']['p50']:.2f}s | {v101_data['statistics']['p50']:.2f}s | {p50_improvement:+.1f}% | {"âœ…" if p50_improvement > 0 else "âŒ"} |
| **P95** | {v100_data['statistics']['p95']:.2f}s | {v101_data['statistics']['p95']:.2f}s | {p95_improvement:+.1f}% | {"âœ…" if p95_improvement > 0 else "âŒ"} |
| **å¹³å‡å€¼** | {v100_data['statistics']['mean_time']:.2f}s | {v101_data['statistics']['mean_time']:.2f}s | {mean_improvement:+.1f}% | {"âœ…" if mean_improvement > 0 else "âŒ"} |
| **æœ€å°å€¼** | {v100_data['statistics']['min_time']:.2f}s | {v101_data['statistics']['min_time']:.2f}s | - | - |
| **æœ€å¤§å€¼** | {v100_data['statistics']['max_time']:.2f}s | {v101_data['statistics']['max_time']:.2f}s | - | - |

## ğŸ¯ ä¸»è¦ç™¼ç¾

### 1. æ•´é«”æ€§èƒ½æ”¹å–„
{"âœ… **é¡¯è‘—æ”¹å–„**" if p50_improvement > 10 else "âš ï¸ **ä¸­åº¦æ”¹å–„**" if p50_improvement > 0 else "âŒ **æ€§èƒ½é€€æ­¥**"}
- P50 å›æ‡‰æ™‚é–“æ”¹å–„ {abs(p50_improvement):.1f}%
- æ•´é«”å›æ‡‰æ™‚é–“å¾ {v100_data['statistics']['p50']:.2f}s é™è‡³ {v101_data['statistics']['p50']:.2f}s
- æ¸›å°‘ç´„ {(v100_data['statistics']['p50'] - v101_data['statistics']['p50']):.2f} ç§’çš„ç­‰å¾…æ™‚é–“

### 2. ä¸€è‡´æ€§åˆ†æ
- P95 æ”¹å–„ {abs(p95_improvement):.1f}%
- æœ€å·®æƒ…æ³å¾ {v100_data['statistics']['max_time']:.2f}s è®Šç‚º {v101_data['statistics']['max_time']:.2f}s
- {"âœ… æ€§èƒ½æ›´åŠ ç©©å®š" if abs(v101_data['statistics']['max_time'] - v101_data['statistics']['min_time']) < abs(v100_data['statistics']['max_time'] - v100_data['statistics']['min_time']) else "âš ï¸ æ€§èƒ½æ³¢å‹•ç›¸ä¼¼"}

### 3. Resume Structure Analyzer å„ªåŒ–æ•ˆæœ
v1.0.1 ç‰ˆæœ¬çš„æ”¹é€²ï¼š
- æ›´ç²¾ç°¡çš„ prompt è¨­è¨ˆ
- å„ªåŒ–çš„ JSON çµæ§‹å®šç¾©
- æ”¹é€²çš„å¤šèªè¨€æ”¯æ´

## ğŸ“ˆ è©³ç´°æ¯”è¼ƒ

### æœ€ä½³è¡¨ç¾æ¡ˆä¾‹ (v1.0.1)
"""

    # Add top 5 performers for v1.0.1
    v101_sorted = sorted(v101_data['test_results'], key=lambda x: x['total_time'])[:5]
    for i, result in enumerate(v101_sorted, 1):
        report += f"\n{i}. {result['company']} - {result['position']}: {result['total_time']:.2f}s"

    report += "\n\n### æœ€å·®è¡¨ç¾æ¡ˆä¾‹ (v1.0.1)\n"
    v101_worst = sorted(v101_data['test_results'], key=lambda x: x['total_time'], reverse=True)[:5]
    for i, result in enumerate(v101_worst, 1):
        report += f"\n{i}. {result['company']} - {result['position']}: {result['total_time']:.2f}s"

    report += f"""

## ğŸ’¡ å„ªåŒ–å»ºè­°

åŸºæ–¼æ¸¬è©¦çµæœï¼Œä»¥ä¸‹æ˜¯é€²ä¸€æ­¥å„ªåŒ–çš„å»ºè­°ï¼š

1. **ç¹¼çºŒå„ªåŒ– Prompt**
   - v1.0.1 é¡¯ç¤ºäº† prompt å„ªåŒ–çš„æ•ˆæœ
   - å¯è€ƒæ…®é€²ä¸€æ­¥ç°¡åŒ–æŒ‡ä»¤ï¼Œæ¸›å°‘ token ä½¿ç”¨

2. **å·®è·åˆ†æä»æ˜¯ä¸»è¦ç“¶é ¸**
   - é›–ç„¶çµæ§‹åˆ†ææœ‰æ”¹å–„ï¼Œä½†æ•´é«”æ™‚é–“ä¸»è¦ä»å—å·®è·åˆ†æå½±éŸ¿
   - å»ºè­°å„ªå…ˆå„ªåŒ–å·®è·åˆ†æçš„ LLM èª¿ç”¨

3. **è€ƒæ…®å¿«å–ç­–ç•¥**
   - å°ç›¸ä¼¼çš„è·ç¼ºæè¿°å¯¦æ–½æ™ºèƒ½å¿«å–
   - æ¸›å°‘é‡è¤‡çš„ LLM èª¿ç”¨

## ğŸ“Š çµè«–

Resume Structure Analyzer v1.0.1 å‡ç´šå¸¶ä¾†äº†{"**é¡¯è‘—çš„æ€§èƒ½æ”¹å–„**" if p50_improvement > 10 else "**ä¸€å®šç¨‹åº¦çš„æ€§èƒ½æ”¹å–„**" if p50_improvement > 0 else "**æœªé”é æœŸçš„æ”¹å–„**"}ï¼š

- âœ… P50 æ”¹å–„ {abs(p50_improvement):.1f}%
- âœ… P95 æ”¹å–„ {abs(p95_improvement):.1f}%
- âœ… å¹³å‡å›æ‡‰æ™‚é–“æ¸›å°‘ {abs(mean_improvement):.1f}%

{"å»ºè­°å°‡ v1.0.1 ä¿æŒç‚ºç”Ÿç”¢ç’°å¢ƒç‰ˆæœ¬ã€‚" if p50_improvement > 0 else "å»ºè­°é€²ä¸€æ­¥èª¿æŸ¥æ€§èƒ½å•é¡Œã€‚"}

---

## é™„éŒ„ï¼šæ¸¬è©¦æª”æ¡ˆ

- v1.0.0 çµæœ: `performance_results_20250816_133242.json`
- v1.0.1 çµæœ: `performance_v101_20250816_143848.json`
- æ¯”è¼ƒåœ–è¡¨: `comparison_chart_[timestamp].png`

*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M')}*
"""

    # Save report
    output_dir = Path('docs/issues/index-cal-gap-analysis-evolution/performance')
    output_file = output_dir / 'PERFORMANCE_COMPARISON_REPORT.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… Comparison report saved to: {output_file}")
    return output_file

def main():
    """Main execution"""
    print("ğŸ” Loading test results...")
    v100_data, v101_data = load_test_results()

    print("ğŸ“Š Creating comparison chart...")
    chart_file = create_comparison_chart(v100_data, v101_data)

    print("ğŸ“ Creating comparison report...")
    report_file = create_comparison_report(v100_data, v101_data)

    print("\n" + "="*60)
    print("COMPARISON SUMMARY")
    print("="*60)

    # Print key improvements
    p50_improvement = ((v100_data['statistics']['p50'] - v101_data['statistics']['p50']) / v100_data['statistics']['p50']) * 100
    p95_improvement = ((v100_data['statistics']['p95'] - v101_data['statistics']['p95']) / v100_data['statistics']['p95']) * 100

    print(f"P50: {v100_data['statistics']['p50']:.2f}s â†’ {v101_data['statistics']['p50']:.2f}s ({p50_improvement:+.1f}%)")
    print(f"P95: {v100_data['statistics']['p95']:.2f}s â†’ {v101_data['statistics']['p95']:.2f}s ({p95_improvement:+.1f}%)")

    if p50_improvement > 0:
        print("\nâœ… Performance IMPROVED with v1.0.1!")
    else:
        print("\nâš ï¸ Performance did not improve as expected.")

    print("\nFiles created:")
    print(f"  - {chart_file}")
    print(f"  - {report_file}")

if __name__ == "__main__":
    main()

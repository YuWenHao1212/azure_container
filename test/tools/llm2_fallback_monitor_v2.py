"""
LLM2 Fallback Monitor Tool V2 - Enhanced Detection
Áî®ÊñºÊõ¥Ê∫ñÁ¢∫Âú∞Áõ£Êéß Resume Tailoring API ‰∏≠ LLM2 ÁöÑ fallback ÁôºÁîüÁéá

Tool ID: TOOL-LLM2-MON-02
Version: 2.0
Changes: 
- Â¢ûÂº∑‰∫ÜfallbackÊ™¢Ê∏¨ÈÇèËºØ
- Ê™¢Ê∏¨Á©∫ÂÖßÂÆπËß∏ÁôºÁöÑfallback
- ÊØîÂ∞çÂéüÂßãÁ∞°Ê≠∑ÂÖßÂÆπ
- Êõ¥Ë©≥Á¥∞ÁöÑË®∫Êñ∑Ëº∏Âá∫
"""

import argparse
import json
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import requests
from bs4 import BeautifulSoup

# Configuration
DEFAULT_API_URL = "https://airesumeadvisor-api-production.calmisland-ea7fe91e.japaneast.azurecontainerapps.io/api/v1/tailor-resume"
DEFAULT_API_KEY = os.getenv("CONTAINER_APP_API_KEY", "hUjQoHxaWoUUvqGxD4eMFYpT4dYXyFmgIK0fwtEePFk")

# Test data - ‰ΩøÁî®ÂâçÁ´ØÁõ∏ÂêåÁöÑÊ∏¨Ë©¶Ë≥áÊñô
ORIGINAL_RESUME = """<h3>Personal Summary:</h3> <p>1. As a Business Administration Manager, I possess extensive experience in data processing, visualization, and interpretation, aiding executive management in making informed decisions. <br>2. With 10 years of hands-on experience in project management, I am proficient in overseeing projects within fast-paced consumer and sophisticated automotive industries. <br>3. My vision is to create value for my community alongside individuals who uphold shared values of respect, freedom, and integrity.</p> <h3>Work Experience:</h3> <p><strong>Business Administration Manager</strong><br><strong>ÂèãÈÅîÂÖâÈõª </strong><em>, Âè∞ÁÅ£ Taiwan Êñ∞Á´πÂ∏Ç </em><br><em> Jul 2022- Apr 2025 </em><br>- Execute advanced data processing and visualization tasks with a focus on the Chinese automotive and IT display sectors, providing pivotal support for executive decision-making. This involves a comprehensive review of business unit performance, incorporating in-depth analysis of financial income statements and data outcomes. The integrated approach not only reveals the overall business operations but also aids in strategic decision-making and promotes corporate growth.<br>- Spearheaded data management projects by collaborating with IT and key departments to deepen the company's insights into its business operations. This effort aims to boost operational efficiency, improve performance metrics, and stimulate innovation across the organization.<br><br><strong>Automotive Project Manager, Deputy Manager</strong><br><strong>ÂèãÈÅîÂÖâÈõª </strong><em>, Âè∞ÁÅ£ Êñ∞Á´πÁ∏£Â∏Ç </em><br><em> May 2019- Jun 2022 </em><br>- RFQ Management and Financial Evaluation: Oversee the Request for Quotation (RFQ) processes for automotive Original Equipment Manufacturers (OEMs), assess project budgets and profit &amp; loss statements, and critically review quotations to guarantee project profitability.<br>- Customer Requirements and Solution Alignment: Manage and align customer requirements with internal solutions, ensuring they meet customer needs. Address and secure customer acceptance for any discrepancies.<br>- Customer Engagement and Project Oversight: Maintain consistent communication with customers through regular meetings, discuss unresolved issues, review risk management strategies, and confirm adherence to project timelines.<br><br><strong>Product Management, Department Manager</strong><br><strong>ÂèãÈÅîÂÖâÈõª </strong><em>, Taiwan </em><br><em> Jul 2011- Apr 2019 </em><br>- Led a team of seven project managers overseeing the production of mobile display products at LCD manufacturing sites in Taiwan and Singapore.<br>- Designed and implemented a profit and loss (P&amp;L) database and dashboard for mobile products, enabling effective data visualization and strategic presentations to the Vice President.<br>- Managed the new product development process, ensuring the on-time delivery of prototypes and compliance with customer specifications.<br>- Achieved significant success in product development, highlighted by:<br>Launching the Xiaomi 4.7" model, achieving sales of 23 million units.<br>Introducing the OPPO 5.5" model, with 12 million units sold.<br><br><strong>Customer Service Engineer</strong><br><strong>Novatek Microelectronics Corp </strong><em>, Hsin-Chu, Taiwan </em><br><em> Apr 2007- Jan 2011 </em><br>- Led the local customer service team to support major Chinese phone makers (such as Huawei, ZTE, etc.) in designing Novatek's display IC, which has been adopted and mass-produced in millions of pieces per month.<br>- Built and trained the customer service team in Mainland China, resulting in a 50%+ reduction in troubleshooting time and business trips after the local service team was established.<br><br><strong>Customer Service/Circuit Switch - Senior Engineer</strong><br><strong>NOKIA </strong><em> </em><br><em> Aug 2004- Mar 2007 </em><br>As an account customer service engineer, I represented Nokia's technical team in supporting telecommunication customers such as Far EasTone and Chunghwa Telecom.</p> <h3>Education Background:</h3> <p><strong> Master of Science in Decision Analysis</strong> <br><strong>Minerva University </strong>| Decision Analysis <br><em> Sep 2023 - Jun 2025</em><br>Minerva University was named Most Innovative University in the World consecutively in 2022 and 2023. <br>Master of Science in Decision Analysis (MDA) prepares students to make informed, consequential decisions across all disciplines and employment sectors, including science, education, government, technology, and finance.<br><br><strong> Master</strong> <br><strong>National Tsing Hua University </strong>| Engineering and System Science<br><em> Sep 2001 - Jun 2003</em><br>NA<br><br><strong> B.S</strong> <br><strong>National Tsing Hua University </strong>| Engineering and System Science<br><em> Sep 1997 - Jun 2001</em><br>NA</p> <h3>Personal Project:</h3> <p><strong> AI Resume Advisor</strong> <br>This web-based tool uses artificial intelligence to help job seekers optimize their resumes and increase their chances of landing interviews. Key features include:<br>1. Resume-Job Description Compatibility Analysis<br>2. Customized Resume Suggestions<br>3. ATS (Applicant Tracking System) Optimization Tips<br>4. Keyword Recommendation<br>5. Format and Structure Improvement<br>The AI analyzes the user's resume against their target job description, providing actionable insights to tailor their application for maximum impact.</p> <h3>Certification:</h3> <p><strong> Industrial Analyst</strong><em> issued by APIAA</em><br><strong> Microsoft Project</strong><em> issued by Microsoft</em><br><strong> PMP</strong><em> issued by PMI-OC Project Management Institute Orange County Chapter</em></p> <h3>Skill:</h3> <p>KPI Dashboards;Data Analysis;Decision Analysis;Hypothesis Testing;Experimental Design;Statistic;APQP;Profit &amp; Loss Management;MS project;Jira ;Analytic Problem Solving;R;SQL;Negotiation and Communication;P&amp;L Calculation;8D Problem Solving;MySQL;Python;Automotive SPICE (ASPICE);Project Management;Tableau;Data Visualization;JIRA;Microsoft Project;Data Analytics;Rational DOORS;Polarion</p>"""

JOB_DESCRIPTION = """Established in 1987 and headquartered in Taiwan, TSMC pioneered the pure-play foundry business model with an exclusive focus on manufacturing its customers' products. In 2023, the company served 528 customers with 11,895 products for high performance computing, smartphones, IoT, automotive, and consumer electronics, and is the world's largest provider of logic ICs with annual capacity of 16 million 12-inch equivalent wafers. TSMC operates fabs in Taiwan as well as manufacturing subsidiaries in Washington State, Japan and China, and its ESMC subsidiary plans to begin construction on a fab in Germany in 2024. In Arizona, TSMC is building three fabs, with the first starting 4nm production in 2025, the second by 2028, and the third by the end of the decade. The Sr. HR Data Analyst will be responsible for analyzing large sets of HR data in order to provide insights and recommendations to the HR team and senior management independently or with other HR analysts across global HR team. The role will require a high level of technical expertise in data visualization and analysis, as well as a deep understanding of HR processes and policies.
Job Responsibilities:
1. Act as a Tableau data visualization expert and develop strategic HR dashboards with other domain experts in cross-team projects that enables data-informed decisions to stakeholders.2. Provide guidance, training plans and technical support to other analysts in HR team for Tableau skills development.3. Translate business needs into technical data requirements and work with IT data platform engineers for data preparation.4. Create trust and maintain strong relationships with key stakeholders across the organization.5. Stay up-to-date with industry trends and new analytics features as a technical advocate.
Job Qualifications:
1. Master's degree in HR, Business, Statistics, CS or related field.2. Minimum 5 years of experience in data analysis, with a proven track record of delivering actionable business insights and recommendations.3. Strong technical skills in data analysis tools such as Tableau, Power BI, Superset and SQL.4. Excellent communication skills with the ability to effectively communicate complex data insights to non-technical stakeholders.5. Strong problem-solving skills with the ability to think critically and creatively to solve complex business problems.6. Ability to work independently and manage multiple projects simultaneously.7. Strong attention to details and accuracy.8. Experience in leading and mentoring junior analysts is a plus."""

# Gap Analysis data
GAP_ANALYSIS = {
    "core_strengths": [
        "üèÜ Top Match: Demonstrated expertise in Tableau and advanced data visualization, with direct experience building KPI dashboards and strategic presentations for executive stakeholders.",
        "‚≠ê Key Strength: Robust background in data analysis and decision science, evidenced by a Master's in Decision Analysis and hands-on use of statistical methods, hypothesis testing, and experimental design.",
        "‚≠ê Key Strength: Proven track record leading cross-functional teams and managing complex projects in automotive and IT industries, showcasing strong project management and stakeholder engagement.",
        "üí° Differentiator: Experience designing and implementing profit and loss databases and dashboards, translating business needs into technical data requirements and collaborating with IT for data preparation.",
        "‚úì Additional Asset: History of training and building teams (e.g., customer service team in China), indicating mentoring capability and ability to support analyst development."
    ],
    "key_gaps": [
        "üîß Power BI & Superset Tools: Not evident in resume. If experienced, add immediately. Otherwise, budget 2-3 months to learn Power BI and Superset for HR analytics.",
        "üìö Formal Data Analyst Role Experience: Absent from current resume. Possibly overlooked? Add it, or plan 2-3 months to develop direct HR data analyst experience."
    ],
    "quick_improvements": [
        "üîß Add \"Data Preparation\" to Skills Section: Your experience collaborating with IT and managing data projects implies strong data preparation skills. Add \"Data Preparation for analytics and dashboard development\" to your skills section.",
        "üíº Highlight \"Critical Thinking & Creativity\": Your decision analysis education and analytic problem-solving can be reframed as \"Applied critical thinking and creative solutions for business decision-making\" in your summary or experience bullets.",
        "üíº Mentoring Evidence: Expand on your experience building and training teams (e.g., Novatek and Product Management roles) by adding \"Mentored and developed junior analysts and engineers, fostering skill growth and team performance.\"",
        "üíº Stakeholder Engagement: Reword your project management bullets to explicitly state \"Engaged key stakeholders across business units to align project goals and ensure successful outcomes.\""
    ],
    "covered_keywords": ["SQL", "Tableau", "Data Visualization", "Dashboards", "Insights", "Communication", "Problem Solving", "Project Management"],
    "missing_keywords": ["Power BI", "Superset", "Data Analyst", "Critical Thinking", "Creativity", "Mentoring", "Stakeholder Engagement", "Data Preparation"],
    "coverage_percentage": 47,
    "similarity_percentage": 65
}


def extract_original_sections(html_content: str) -> dict[str, str]:
    """ÂæûÂéüÂßãÁ∞°Ê≠∑‰∏≠ÊèêÂèñÂêÑÂÄãsectionÁöÑÂÖßÂÆπ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    sections = {}

    # ÂÆöÁæ©sectionÊ®ôË®òÂíåÂ∞çÊáâÁöÑÈóúÈçµÂ≠ó
    section_markers = {
        'education': ['Education Background:', 'Education:', 'Academic'],
        'projects': ['Personal Project:', 'Projects:', 'Side Projects:'],
        'certifications': ['Certification:', 'Certifications:', 'Certificates:']
    }

    for section_name, markers in section_markers.items():
        for marker in markers:
            # Â∞ãÊâæÂåÖÂê´Ê®ôË®òÁöÑÂÖÉÁ¥†
            for elem in soup.find_all(string=re.compile(marker, re.IGNORECASE)):
                parent = elem.parent
                if parent:
                    # Áç≤ÂèñË©≤sectionÁöÑÂÖßÂÆπ
                    content = []
                    current = parent
                    while current and current.next_sibling:
                        current = current.next_sibling
                        if hasattr(current, 'text'):
                            text = current.text.strip()
                            if text and not any(m in text for markers_list in section_markers.values() for m in markers_list):
                                content.append(text)
                            elif any(m in text for markers_list in section_markers.values() for m in markers_list):
                                break  # ÈÅáÂà∞‰∏ã‰∏ÄÂÄãsectionÊ®ôË®òÂ∞±ÂÅúÊ≠¢

                    if content:
                        sections[section_name] = ' '.join(content)
                        break

    return sections


def detect_content_similarity(original_text: str, optimized_html: str, threshold: float = 0.8) -> float:
    """Ê™¢Ê∏¨ÂÑ™ÂåñÂæåÁöÑHTML‰∏≠ÊúâÂ§öÂ∞ëÂÖßÂÆπ‰æÜËá™ÂéüÂßãÁ∞°Ê≠∑ÔºàÁõ∏‰ººÂ∫¶Ê™¢Ê∏¨Ôºâ"""
    if not original_text or not optimized_html:
        return 0.0

    # Ê∏ÖÁêÜHTMLÊ®ôÁ±§
    soup = BeautifulSoup(optimized_html, 'html.parser')
    optimized_text = soup.get_text().strip()

    # ÂàÜÂâ≤ÊàêÂè•Â≠êÊàñÁü≠Ë™û
    original_phrases = set(filter(None, re.split(r'[;.\n]', original_text)))
    optimized_phrases = set(filter(None, re.split(r'[;.\n]', optimized_text)))

    # Ë®àÁÆóÁõ∏‰ººÂ∫¶
    if not optimized_phrases:
        return 0.0

    matched = 0
    for opt_phrase in optimized_phrases:
        opt_clean = opt_phrase.strip().lower()
        for orig_phrase in original_phrases:
            orig_clean = orig_phrase.strip().lower()
            # Â¶ÇÊûú80%‰ª•‰∏äÁöÑÊñáÂ≠óÁõ∏ÂêåÔºåË™çÁÇ∫ÊòØ‰æÜËá™ÂéüÂßãÁ∞°Ê≠∑
            if len(opt_clean) > 20 and orig_clean in opt_clean:
                matched += 1
                break

    return matched / len(optimized_phrases)


def analyze_response_enhanced(response_json: dict, original_resume: str) -> dict:
    """Â¢ûÂº∑ÁöÑAPIÈüøÊáâÂàÜÊûêÔºåÂåÖÂê´Â§öÁ®ÆfallbackÊ™¢Ê∏¨ÊñπÊ≥ï"""
    if not response_json.get("success") or "data" not in response_json:
        return None

    data = response_json["data"]

    # 1. Ê™¢Êü•warningsÂ≠óÊÆµÔºàÂéüÂßãÊñπÊ≥ïÔºâ
    warnings = data.get("warnings", [])
    has_warning_fallback = any("LLM2" in w or "fallback" in w.lower() for w in warnings)

    # 2. Ê™¢Êü•optimized_resumeÂÖßÂÆπ
    optimized_resume = data.get("optimized_resume", "")

    # ÊèêÂèñÂéüÂßãÁ∞°Ê≠∑ÁöÑsections
    original_sections = extract_original_sections(original_resume)

    # 3. Ê™¢Ê∏¨ÂêÑsectionÁöÑÁõ∏‰ººÂ∫¶
    section_similarities = {}
    sections_with_fallback = []

    # Ëß£ÊûêÂÑ™ÂåñÂæåÁöÑHTML
    soup = BeautifulSoup(optimized_resume, 'html.parser')

    # Ê™¢Êü•Education section
    education_elem = soup.find('h2', string=re.compile('Education', re.IGNORECASE))
    if education_elem:
        education_content = []
        current = education_elem.next_sibling
        while current and (not hasattr(current, 'name') or current.name != 'h2'):
            if hasattr(current, 'text'):
                education_content.append(current.text)
            current = current.next_sibling if hasattr(current, 'next_sibling') else None

        education_text = ' '.join(education_content)
        if original_sections.get('education'):
            similarity = detect_content_similarity(original_sections['education'], education_text)
            section_similarities['education'] = similarity
            if similarity > 0.7:  # 70%‰ª•‰∏äÁõ∏‰ººÂ∫¶Ë™çÁÇ∫ÊòØfallback
                sections_with_fallback.append('education')

    # Ê™¢Êü•Projects section
    projects_elem = soup.find('h2', string=re.compile('Project', re.IGNORECASE))
    if projects_elem:
        projects_content = []
        current = projects_elem.next_sibling
        while current and (not hasattr(current, 'name') or current.name != 'h2'):
            if hasattr(current, 'text'):
                projects_content.append(current.text)
            current = current.next_sibling if hasattr(current, 'next_sibling') else None

        projects_text = ' '.join(projects_content)
        if original_sections.get('projects'):
            similarity = detect_content_similarity(original_sections['projects'], projects_text)
            section_similarities['projects'] = similarity
            if similarity > 0.7:
                sections_with_fallback.append('projects')

    # Ê™¢Êü•Certifications section
    cert_elem = soup.find('h2', string=re.compile('Certification', re.IGNORECASE))
    if cert_elem:
        cert_content = []
        current = cert_elem.next_sibling
        while current and (not hasattr(current, 'name') or current.name != 'h2'):
            if hasattr(current, 'text'):
                cert_content.append(current.text)
            current = current.next_sibling if hasattr(current, 'next_sibling') else None

        cert_text = ' '.join(cert_content)
        if original_sections.get('certifications'):
            similarity = detect_content_similarity(original_sections['certifications'], cert_text)
            section_similarities['certifications'] = similarity
            if similarity > 0.7:
                sections_with_fallback.append('certifications')

    # 4. Ê™¢Êü•ÊòØÂê¶Áº∫Â∞ëopt-classesÔºàË°®Á§∫Ê≤íÊúâÂÑ™ÂåñÔºâ
    has_opt_classes = "opt-" in optimized_resume

    # 5. Á∂úÂêàÂà§Êñ∑ÊòØÂê¶‰ΩøÁî®‰∫Üfallback
    has_llm2_fallback = (
        has_warning_fallback or
        len(sections_with_fallback) > 0 or
        (not has_opt_classes and len(section_similarities) > 0)
    )

    return {
        "has_llm2_fallback": has_llm2_fallback,
        "fallback_indicators": {
            "warning_based": has_warning_fallback,
            "content_similarity": sections_with_fallback,
            "missing_opt_classes": not has_opt_classes,
            "section_similarities": section_similarities
        },
        "warnings": warnings,
        "sections_analyzed": list(section_similarities.keys())
    }


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Enhanced LLM2 fallback monitor with better detection',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('-n', '--num-tests', type=int, default=10,
                        help='Number of tests to run (default: 10)')
    parser.add_argument('--delay', type=int, default=2,
                        help='Delay in seconds between tests (default: 2)')
    parser.add_argument('--verbose', action='store_true',
                        help='Show detailed output for each test')
    parser.add_argument('--save-responses', action='store_true',
                        help='Save full API responses to files')
    parser.add_argument('--api-url', type=str, default=DEFAULT_API_URL,
                        help='API endpoint URL')
    parser.add_argument('--api-key', type=str, default=DEFAULT_API_KEY,
                        help='API key for authentication')

    return parser.parse_args()


def make_api_request(api_url: str, api_key: str, payload: dict, timeout: int = 60) -> requests.Response:
    """Make API request with error handling"""
    headers = {
        "Content-Type": "application/json",
        "X-API-Key": api_key
    }

    try:
        response = requests.post(api_url, json=payload, headers=headers, timeout=timeout)
        return response
    except requests.exceptions.RequestException as e:
        return None, str(e)


def run_monitor(args):
    """Run the enhanced monitoring tests"""
    # Prepare payload
    payload = {
        "job_description": JOB_DESCRIPTION,
        "original_resume": ORIGINAL_RESUME,
        "original_index": GAP_ANALYSIS,
        "options": {
            "language": "en",
            "include_visual_markers": True,
            "format_version": "v3"
        }
    }

    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(f"/tmp/llm2_fallback_test_v2_{timestamp}")
    if args.save_responses:
        output_dir.mkdir(exist_ok=True)
        print(f"üìÅ Responses will be saved to: {output_dir}")

    print("\n" + "="*80)
    print(f"üîç Enhanced LLM2 Fallback Monitor V2 - Running {args.num_tests} tests")
    print("="*80)
    print(f"API URL: {args.api_url}")
    print("Detection methods: Warnings + Content Similarity + CSS Classes")
    print("")

    results = []
    fallback_count = 0

    # Track fallback types
    fallback_types = {
        "warning_based": 0,
        "content_similarity": 0,
        "missing_opt_classes": 0
    }

    section_fallback_counts = {
        "education": 0,
        "projects": 0,
        "certifications": 0
    }

    for i in range(1, args.num_tests + 1):
        print(f"üîÑ Test #{i}/{args.num_tests}...", end='')

        start_time = time.time()
        response = make_api_request(args.api_url, args.api_key, payload)
        elapsed_time = time.time() - start_time

        if isinstance(response, tuple):  # Error occurred
            print(f" ‚ùå Exception: {response[1]}")
            results.append({
                "test_number": i,
                "error": response[1]
            })
            continue

        if response.status_code != 200:
            print(f" ‚ùå HTTP {response.status_code}")
            results.append({
                "test_number": i,
                "error": f"HTTP {response.status_code}"
            })
            continue

        try:
            result_json = response.json()

            # Save response if requested
            if args.save_responses:
                response_file = output_dir / f"response_{i:03d}.json"
                with open(response_file, "w") as f:
                    json.dump({
                        "request": payload,
                        "response": result_json,
                        "metadata": {
                            "test_number": i,
                            "response_time": elapsed_time
                        }
                    }, f, indent=2)

            # Analyze with enhanced detection
            analysis = analyze_response_enhanced(result_json, ORIGINAL_RESUME)

            if analysis and analysis["has_llm2_fallback"]:
                fallback_count += 1
                print(f" ‚ö†Ô∏è FALLBACK DETECTED ({elapsed_time:.1f}s)")

                # Track fallback types
                indicators = analysis["fallback_indicators"]
                if indicators["warning_based"]:
                    fallback_types["warning_based"] += 1
                if indicators["content_similarity"]:
                    fallback_types["content_similarity"] += 1
                    for section in indicators["content_similarity"]:
                        section_fallback_counts[section] += 1
                if indicators["missing_opt_classes"]:
                    fallback_types["missing_opt_classes"] += 1

                if args.verbose:
                    print("   üìä Fallback details:")
                    print(f"      - Warning-based: {indicators['warning_based']}")
                    print(f"      - Content similarity: {indicators['content_similarity']}")
                    print(f"      - Section similarities: {indicators['section_similarities']}")
                    print(f"      - Missing opt-classes: {indicators['missing_opt_classes']}")
            else:
                print(f" ‚úÖ No fallback ({elapsed_time:.1f}s)")

            results.append({
                "test_number": i,
                "has_llm2_fallback": analysis["has_llm2_fallback"] if analysis else False,
                "response_time": elapsed_time,
                "analysis": analysis
            })

        except Exception as e:
            print(f" ‚ùå Analysis error: {e}")
            results.append({
                "test_number": i,
                "error": str(e)
            })

        # Delay between tests
        if i < args.num_tests:
            time.sleep(args.delay)

    # Summary
    print("\n" + "="*80)
    print("üìä TEST SUMMARY")
    print("="*80)

    successful_tests = [r for r in results if "error" not in r]
    print(f"Total tests: {args.num_tests}")
    print(f"Successful: {len(successful_tests)}")
    print(f"Failed: {args.num_tests - len(successful_tests)}")

    if successful_tests:
        fallback_rate = (fallback_count / len(successful_tests)) * 100
        print(f"\nüéØ LLM2 Fallback Rate: {fallback_rate:.1f}% ({fallback_count}/{len(successful_tests)})")

        print("\nüìà Fallback Detection Methods:")
        print(f"   - Warning-based: {fallback_types['warning_based']} occurrences")
        print(f"   - Content similarity: {fallback_types['content_similarity']} occurrences")
        print(f"   - Missing opt-classes: {fallback_types['missing_opt_classes']} occurrences")

        print("\nüìë Section-specific Fallbacks:")
        for section, count in section_fallback_counts.items():
            if count > 0:
                section_rate = (count / len(successful_tests)) * 100
                print(f"   - {section.capitalize()}: {count} times ({section_rate:.1f}%)")

        # Average response time
        avg_time = sum(r["response_time"] for r in successful_tests) / len(successful_tests)
        print(f"\n‚è±Ô∏è Average response time: {avg_time:.2f}s")

    # Save summary
    if args.save_responses:
        summary_file = output_dir / "summary.json"
        with open(summary_file, "w") as f:
            json.dump({
                "test_datetime": datetime.now().isoformat(),
                "configuration": {
                    "num_tests": args.num_tests,
                    "delay_seconds": args.delay,
                    "api_url": args.api_url
                },
                "summary": {
                    "total_tests": args.num_tests,
                    "successful": len(successful_tests),
                    "llm2_fallbacks": fallback_count,
                    "fallback_rate": f"{fallback_rate:.1f}%" if successful_tests else "N/A",
                    "fallback_types": fallback_types,
                    "section_fallbacks": section_fallback_counts
                },
                "detailed_results": results
            }, f, indent=2)
        print(f"\nüíæ Summary saved to: {summary_file}")


if __name__ == "__main__":
    args = parse_arguments()
    run_monitor(args)
